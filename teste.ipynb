{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332f6712",
   "metadata": {},
   "source": [
    "### Configuração do ambiente de avaliação/inferência e hiperparâmetros de geração\n",
    "\n",
    "Este bloco prepara, Liberum, as **constantes e parâmetros** usados na fase de **avaliação** (ou inferência controlada) do modelo ajustado, com foco em **reprodutibilidade** e **baixo consumo de VRAM** (≈8 GB).\n",
    "\n",
    "**Entradas**\n",
    "- N/A (somente define variáveis/constantes para uso posterior).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Imports e seed**  \n",
    "   - Carrega bibliotecas de NLP/treinamento (Transformers, Datasets, PEFT, etc.).  \n",
    "   - Define `set_seed(42)` para reprodutibilidade em geração e amostragem de subsets.\n",
    "2. **Modelos e dados**  \n",
    "   - `BASE_MODEL`: nome do modelo base usado como referência nas métricas.  \n",
    "   - `ADAPTER_PATH`: caminho do **adapter LoRA** treinado (saída do SFT).  \n",
    "   - `DATA_PATH`: caminho do dataset de teste (JSON com `title`, `content`).\n",
    "3. **Idioma de referência**  \n",
    "   - `LANG = \"en\"` (ajuste para `\"pt\"` se suas referências/respostas-alvo estiverem em português).\n",
    "4. **Limites amigáveis para 8 GB de VRAM**  \n",
    "   - `USE_4BIT = True`: habilita configuração pensada para quantização 4-bit (será usada na carga do modelo).  \n",
    "   - `MAX_LEN = 512`: limita o comprimento do prompt/entrada para caber em GPUs menores.  \n",
    "   - `VAL_SAMPLES` e `TEST_SAMPLES`: tamanhos de amostra para acelerar validação/teste.\n",
    "5. **Parâmetros de geração** (`GEN_KWARGS`)  \n",
    "   - `max_new_tokens=256`: controla o tamanho máximo da resposta gerada.  \n",
    "   - `do_sample=False`: usa **decodificação gulosa** (determinística).  \n",
    "   - `temperature=0.2`, `top_p=0.9`: definidos, mas **não têm efeito quando `do_sample=False`** (ficam inativos).\n",
    "\n",
    "**Saídas**\n",
    "- Variáveis globais configuradas (`BASE_MODEL`, `ADAPTER_PATH`, `DATA_PATH`, `LANG`, `USE_4BIT`, `MAX_LEN`, `VAL_SAMPLES`, `TEST_SAMPLES`, `GEN_KWARGS`) prontas para o próximo bloco (carregamento do modelo/adapter e execução das métricas).\n",
    "\n",
    "**Observações**\n",
    "- Se seu conjunto de referências estiver em **português**, altere `LANG` para `\"pt\"` para alinhar a avaliação.  \n",
    "- Considere aumentar `MAX_LEN` para 1024 apenas se a GPU suportar sem **OOM**.  \n",
    "- Para respostas mais criativas/variadas, mude para `do_sample=True` e então `temperature/top_p` passam a ter efeito.  \n",
    "- Garanta que `ADAPTER_PATH` aponte para o diretório correto do LoRA salvo (ex.: `./models/out-sft/final`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8198311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed)\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "BASE_MODEL   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   \n",
    "ADAPTER_PATH = \"./models/out-sft/final\"              \n",
    "DATA_PATH    = \"./data/tst.json\"  \n",
    "\n",
    "LANG = \"en\"   # \"en\" ou \"pt\"\n",
    "\n",
    "\n",
    "USE_4BIT = True\n",
    "MAX_LEN  = 512            \n",
    "VAL_SAMPLES  = 512      \n",
    "TEST_SAMPLES = 512\n",
    "\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7d3b0",
   "metadata": {},
   "source": [
    "### Carregamento do modelo base + aplicação do adapter LoRA (quantização 4-bit) para inferência\n",
    "\n",
    "Este bloco monta o **pipeline de inferência** com foco em **baixo uso de VRAM** (8 GB), habilitando quantização 4-bit via *bitsandbytes* e aplicando o **adapter LoRA** treinado sobre o modelo base.\n",
    "\n",
    "**Entradas**\n",
    "- `BASE_MODEL`: identificador do modelo base no Hugging Face Hub.\n",
    "- `ADAPTER_PATH`: diretório do adapter LoRA salvo após o SFT.\n",
    "- `USE_4BIT`: habilita/desabilita quantização 4-bit.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Configuração 4-bit (opcional)**  \n",
    "   - Cria `BitsAndBytesConfig` com `nf4`, *double quant* e `compute_dtype=torch.float16`, reduzindo significativamente a VRAM.\n",
    "2. **Tokenizer**  \n",
    "   - `padding_side=\"left\"` (recomendado para *causal LM* em geração batelada).  \n",
    "   - Define `pad_token = eos_token` caso inexistente, evitando avisos/erros no *batching*.\n",
    "3. **Dicas de performance**  \n",
    "   - `torch.backends.cuda.matmul.allow_tf32 = True` habilita TF32 em GPUs Ampere/Hopper para *matmul* em FP32 (acelera operações compatíveis).  \n",
    "   - `torch.set_grad_enabled(False)` desativa *autograd* (inferência pura).\n",
    "4. **Carregamento do modelo base**  \n",
    "   - `AutoModelForCausalLM.from_pretrained(...)` com `torch_dtype` ajustado ao hardware (FP16 com CUDA; FP32 na CPU), `quantization_config=bnb_config` e `device_map=\"auto\"` (coloca o modelo automaticamente no(s) dispositivo(s) disponível(is)).\n",
    "5. **Aplicação do adapter LoRA**  \n",
    "   - `PeftModel.from_pretrained(base_model, ADAPTER_PATH)` carrega os pesos LoRA no topo do modelo base.  \n",
    "   - `model.eval()` coloca o modelo em modo de avaliação (desativa *dropout*).\n",
    "6. **Confirmação de dispositivo**  \n",
    "   - Imprime o `device` do primeiro parâmetro do modelo para checagem rápida.\n",
    "\n",
    "**Saídas**\n",
    "- Objeto `model` pronto para geração (ex.: `model.generate(...)`) com quantização 4-bit (se habilitada) e LoRA aplicado.\n",
    "- `tokenizer` configurado com *left padding*.\n",
    "\n",
    "**Observações**\n",
    "- A quantização 4-bit depende do pacote **`bitsandbytes`**; garanta que esteja instalado e compatível com sua GPU/driver.  \n",
    "- TF32 acelera operações FP32; como a inferência aqui usa FP16, o ganho pode ser limitado, mas a flag é inofensiva.  \n",
    "- Se ocorrer **OOM**, reduza `MAX_LEN`/`max_new_tokens` ou desative camadas não essenciais.  \n",
    "- Para máxima compatibilidade, mantenha o `tokenizer` e o `model` do **mesmo checkpoint base** usado no treinamento do adapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd8e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Quantização 4-bit (recomendado em 8GB)\n",
    "bnb_config = None\n",
    "if USE_4BIT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, padding_side=\"left\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dicas de perf\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890725c",
   "metadata": {},
   "source": [
    "### Preparação do dataset de avaliação e criação de prompts em formato de chat\n",
    "\n",
    "Este bloco, Liberum, carrega o dataset local a partir de JSON, realiza uma divisão **estratificada por seed** em `train/validation/test` e constrói um formato de **chat** (system/user → assistant) para avaliação/geração, produzindo pares `{prompt, reference}`.\n",
    "\n",
    "**Entradas**\n",
    "- `DATA_PATH`: caminho para um JSON com colunas `title` e `content`.\n",
    "- `LANG`: idioma alvo dos prompts (`\"en\"` ou `\"pt\"`).\n",
    "- `tokenizer`: já carregado previamente (usado para `apply_chat_template`).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Carga do dataset**  \n",
    "   - `load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")` lê o JSON como um único split.\n",
    "2. **Split determinístico**  \n",
    "   - `train_test_split(test_size=0.1, seed=42)` separa ~10% para **test**.  \n",
    "   - Nova divisão no restante para criar **validation** (~10% do restante ≈ 0.1 total) e **train** (~80% total), via `train_test_split(test_size=0.1111, seed=42)`.  \n",
    "   - Resultado final aproximado: **80% train / 10% validation / 10% test**.\n",
    "3. **Template de chat**  \n",
    "   - `build_chat_prompt(title, lang)` cria mensagens `system` e `user` diferentes conforme `LANG`:\n",
    "     - EN: “Write a detailed description…”  \n",
    "     - PT: “Escreva uma descrição detalhada…”\n",
    "   - `tokenizer.apply_chat_template(..., tokenize=False, add_generation_prompt=True)` serializa as mensagens no formato esperado pelo modelo e adiciona o prefixo para a resposta do **assistant**.\n",
    "4. **Mapeamento para avaliação**  \n",
    "   - `format_example_chat` transforma cada exemplo em:\n",
    "     - `prompt`: texto de entrada no formato de chat (com título e instruções).\n",
    "     - `reference`: o `content` original, usado como referência para métricas/checagem de qualidade.\n",
    "   - Aplica o `map` em cada split e monta `dataset_chat`.\n",
    "5. **Inspeção**  \n",
    "   - Visualiza o primeiro item de `validation` para checagem rápida do schema.\n",
    "\n",
    "**Saídas**\n",
    "- `dataset`: `DatasetDict` com `train`, `validation`, `test`.\n",
    "- `dataset_chat`: `DatasetDict` com campos `prompt` e `reference` em cada split.\n",
    "- Um exemplo impresso de `dataset_chat[\"validation\"][0]` para verificação.\n",
    "\n",
    "**Observações**\n",
    "- A seed (`42`) garante **reprodutibilidade** do split.  \n",
    "- `add_generation_prompt=True` é importante para modelos chat-based que esperam o marcador de início da fala do **assistant**.  \n",
    "- Certifique-se de que os títulos (`title`) sejam informativos; prompts genéricos podem degradar a avaliação.  \n",
    "- O `reference` serve como “ground truth” textual; métricas do tipo ROUGE/BLEU/BERTScore podem ser aplicadas em etapa posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9a3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 776199/776199 [01:08<00:00, 11319.11 examples/s]\n",
      "Map: 100%|██████████| 97014/97014 [00:08<00:00, 11233.19 examples/s]\n",
      "Map: 100%|██████████| 97024/97024 [00:08<00:00, 11292.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'uid': 'B00CHH2SG8',\n",
       " 'title': 'Crazy Bananas 6 Pieces Candy Scented Nail Polish Set, (Cherry, Blueberry, Grape, Strawberry, Banana, Apple)',\n",
       " 'content': 'Crazy Bananas 6 piece candy scented nail polish. Kids fun nail polish. 0.24 fluid ounce each. Great gift.',\n",
       " 'target_ind': [46466,\n",
       "  52639,\n",
       "  56909,\n",
       "  102146,\n",
       "  107626,\n",
       "  110728,\n",
       "  113886,\n",
       "  113964,\n",
       "  276873,\n",
       "  304324,\n",
       "  307464,\n",
       "  344542,\n",
       "  380682,\n",
       "  381982,\n",
       "  392963,\n",
       "  503975,\n",
       "  514450,\n",
       "  558488,\n",
       "  561824,\n",
       "  577354,\n",
       "  587446,\n",
       "  679152,\n",
       "  728168,\n",
       "  1069311],\n",
       " 'target_rel': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'prompt': '<|system|>\\nYou are a helpful assistant that writes detailed product descriptions from a given product title.</s>\\n<|user|>\\nTITLE: Crazy Bananas 6 Pieces Candy Scented Nail Polish Set, (Cherry, Blueberry, Grape, Strawberry, Banana, Apple)\\nQuestion: Write a detailed description for this product.</s>\\n<|assistant|>\\n',\n",
       " 'reference': 'Crazy Bananas 6 piece candy scented nail polish. Kids fun nail polish. 0.24 fluid ounce each. Great gift.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega dataset local (JSON com colunas: title, content)\n",
    "raw = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "# Split train/val/test\n",
    "dataset = raw.train_test_split(test_size=0.1, seed=42)\n",
    "tmp = dataset[\"train\"].train_test_split(test_size=0.1111, seed=42)  # ~10% val, ~80% train\n",
    "dataset = DatasetDict({\n",
    "    \"train\": tmp[\"train\"],\n",
    "    \"validation\": tmp[\"test\"],\n",
    "    \"test\": dataset[\"test\"]\n",
    "})\n",
    "\n",
    "def build_chat_prompt(title: str, lang: str = \"en\") -> str:\n",
    "    if lang == \"en\":\n",
    "        system = \"You are a helpful assistant that writes detailed product descriptions from a given product title.\"\n",
    "        user   = f\"TITLE: {title}\\nQuestion: Write a detailed description for this product.\"\n",
    "    else:  # pt\n",
    "        system = \"Você é um assistente que escreve uma descrição detalhada do produto a partir do título fornecido.\"\n",
    "        user   = f\"TÍTULO: {title}\\nPergunta: Escreva uma descrição detalhada para este produto.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "    # add_generation_prompt=True adiciona o prefixo para a resposta do assistant\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def format_example_chat(example, lang=\"en\"):\n",
    "    return {\n",
    "        \"prompt\": build_chat_prompt(example[\"title\"], lang=lang),\n",
    "        \"reference\": example[\"content\"]\n",
    "    }\n",
    "\n",
    "dataset_chat = DatasetDict({\n",
    "    split: ds.map(lambda ex: format_example_chat(ex, lang=LANG))\n",
    "    for split, ds in dataset.items()\n",
    "})\n",
    "\n",
    "# Visualiza um exemplo\n",
    "dataset_chat[\"validation\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526b9c3",
   "metadata": {},
   "source": [
    "### Tokenização com máscara de rótulos (targets-only) e cálculo de Perplexidade\n",
    "\n",
    "Este bloco, Liberum, prepara os exemplos no formato **[prompt + referência]** para avaliar o modelo medindo **Perplexidade** apenas sobre o **alvo (referência)** — o **prompt é ignorado** na perda via máscara de rótulos `-100`.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[\"validation\"]`: com campos `prompt` e `reference`.\n",
    "- `tokenizer`, `model`, `MAX_LEN`, `tokenizer.pad_token_id`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **`tokenize_for_ppl_only_target(batch)`**\n",
    "   - Tokeniza `prompt` (`p_ids`) **sem** adicionar *special tokens* (o chat template já contém).\n",
    "   - Tokeniza a `reference` antecedida por um espaço (`\" \" + r`) → ajuda na junção de tokens.\n",
    "   - Concatena `ids = p_ids + r_ids` e **trunca** para `MAX_LEN`.\n",
    "   - Cria `attention_mask` = 1 para todos os tokens válidos.\n",
    "   - Monta `labels` com **`-100` para os tokens do prompt** (ignorados na loss) e **`r_ids`** como alvos. Ajusta o comprimento para casar com `ids`.\n",
    "   - Retorna `input_ids`, `attention_mask`, `labels`.\n",
    "2. **Mapeamento do split de validação**\n",
    "   - Aplica a função acima com `batched=True`; remove colunas originais, gerando `tok_val_masked`.\n",
    "3. **Padding dinâmico custom (`pad_to_max`)**\n",
    "   - Calcula o comprimento máximo do lote e **preenche**:\n",
    "     - `input_ids` com `pad_id`.\n",
    "     - `attention_mask` com `0` no padding.\n",
    "     - `labels` com `-100` no padding (mantém máscara de perda coerente).\n",
    "4. **`compute_perplexity_masked(tok_dataset, batch_size=4)`**\n",
    "   - Cria `DataLoader` com `collate_fn` que chama `pad_to_max`.\n",
    "   - Faz **inferência sem gradiente** (`torch.no_grad()`), coletando `out.loss` por batch.\n",
    "   - Calcula `mean_loss` e `perplexity = exp(clamp(mean_loss, 20.0))` (o clamp evita overflow numérico).\n",
    "\n",
    "**Saídas**\n",
    "- Dicionário com métricas: `{\"val_loss\": <float>, \"perplexity\": <float>}`.\n",
    "- `tok_val_masked`: dataset tokenizado e rotulado para avaliação.\n",
    "\n",
    "**Observações**\n",
    "- **Importante**: usar `-100` nos rótulos do prompt garante que a loss/perplexidade meça **apenas** a qualidade da geração da resposta (referência), não da instrução.\n",
    "- O espaço antes da referência evita junção indesejada de tokens (wordpiece/BPE).\n",
    "- Se ocorrer **OOM**, reduza `MAX_LEN` ou `batch_size`.\n",
    "- O padding manual garante consistência entre `input_ids`, `attention_mask` e `labels` já mascarados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/97014 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2052 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 97014/97014 [00:22<00:00, 4251.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_for_ppl_only_target(batch):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    refs    = batch[\"reference\"]\n",
    "\n",
    "    input_ids_list, attn_masks_list, labels_list = [], [], []\n",
    "    for p, r in zip(prompts, refs):\n",
    "        # tokenize sem adicionar specials (chat template já contém tokens de chat)\n",
    "        p_ids = tokenizer(p, add_special_tokens=False).input_ids\n",
    "        r_ids = tokenizer(\" \" + r, add_special_tokens=False).input_ids  # espaço anteposto ajuda\n",
    "        # concat e trunca\n",
    "        ids = (p_ids + r_ids)[:MAX_LEN]\n",
    "        am  = [1] * len(ids)\n",
    "        # labels: -100 no prompt; r_ids como alvo\n",
    "        labels = ([-100] * min(len(p_ids), len(ids))) + r_ids\n",
    "        labels = labels[:len(ids)]  # garante mesmo comprimento que ids\n",
    "\n",
    "        input_ids_list.append(ids)\n",
    "        attn_masks_list.append(am)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attn_masks_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "tok_val_masked = dataset_chat[\"validation\"].map(\n",
    "    tokenize_for_ppl_only_target, batched=True,\n",
    "    remove_columns=dataset_chat[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "# Padding dinâmico custom (como já temos labels prontos com -100)\n",
    "def pad_to_max(batch, pad_id, label_pad=-100):\n",
    "    maxlen = max(len(x) for x in batch[\"input_ids\"])\n",
    "    def pad(seq, fill, L): return seq + [fill] * (L - len(seq))\n",
    "    input_ids = [pad(x, pad_id, maxlen) for x in batch[\"input_ids\"]]\n",
    "    attn_mask = [pad(x, 0,      maxlen) for x in batch[\"attention_mask\"]]\n",
    "    labels    = [pad(x, label_pad, maxlen) for x in batch[\"labels\"]]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attn_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "def compute_perplexity_masked(tok_dataset, batch_size=4):\n",
    "    loader = DataLoader(\n",
    "        tok_dataset, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=lambda b: pad_to_max(\n",
    "            {k: [d[k] for d in b] for k in b[0].keys()},\n",
    "            pad_id=tokenizer.pad_token_id, label_pad=-100\n",
    "        )\n",
    "    )\n",
    "    losses = []\n",
    "    for batch in tqdm(loader, desc=\"Perplexidade (targets-only)\"):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "        losses.append(out.loss.item())\n",
    "    mean_loss = float(np.mean(losses))\n",
    "    ppl = float(math.exp(min(mean_loss, 20.0)))\n",
    "    return {\"val_loss\": mean_loss, \"perplexity\": ppl}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25b23a",
   "metadata": {},
   "source": [
    "### Geração em lote e avaliação automática com ROUGE, SacreBLEU, chrF e METEOR\n",
    "\n",
    "Este bloco, Liberum, executa **inferência batelada** a partir dos `prompts` e calcula métricas de **sobreposição n-gram** e **caracteres** para avaliar a qualidade das saídas do modelo em relação às referências.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[<split>]` com campos `prompt` e `reference`.\n",
    "- `tokenizer`, `model`, `MAX_LEN`, `GEN_KWARGS`, `LANG`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Carregamento de métricas**  \n",
    "   - `rouge`, `sacrebleu`, `chrf` via `evaluate.load`.  \n",
    "   - `meteor` é opcional (controlado por `HAS_METEOR` — pode falhar por dependências).\n",
    "2. **Geração em lote (`generate_batch`)**  \n",
    "   - Tokeniza `prompts` com **padding** e **truncation** em `MAX_LEN`.  \n",
    "   - Usa `model.generate` com `GEN_KWARGS`, definindo `eos_token_id` e `pad_token_id`.  \n",
    "   - **Pós-processamento**: aplica heurística simples para extrair apenas a resposta do **assistant**:\n",
    "     - Divide pelo marcador da pergunta (`\"Question:\"` ou `\"Pergunta:\"` conforme `LANG`).  \n",
    "     - Tenta cortar após `\"Assistant:\"` se existir no template.  \n",
    "     - Faz `strip()` do texto final.\n",
    "3. **Avaliação de um split (`evaluate_split`)**  \n",
    "   - Itera sobre o conjunto em minibatches (16 exemplos), chamando `generate_batch`.  \n",
    "   - Agrega hipóteses (`hyps`) e referências (`refs`).  \n",
    "   - Calcula:\n",
    "     - **ROUGE** (`rouge1`, `rouge2`, `rougeL`) com *stemming*.  \n",
    "     - **SacreBLEU** (formato requer `[[ref]]` por hipótese).  \n",
    "     - **chrF** (nível de caracteres).  \n",
    "     - **METEOR** (se disponível).\n",
    "   - Retorna dicionário com as pontuações agregadas.\n",
    "\n",
    "**Saídas**\n",
    "- Dicionário com métricas:  \n",
    "  `{\"rouge1\", \"rouge2\", \"rougeL\", \"sacrebleu\", \"chrf\", \"meteor\"}`.\n",
    "\n",
    "**Observações**\n",
    "- A heurística de recorte pode variar conforme o **chat template** do modelo; ajuste as âncoras se o checkpoint usar marcadores diferentes.  \n",
    "- Se `do_sample=False` em `GEN_KWARGS`, a geração é **determinística** (útil para comparabilidade entre runs).  \n",
    "- Ajuste `MAX_LEN` caso ocorram truncamentos severos do prompt (pode afetar as métricas).  \n",
    "- `METEOR` pode exigir dependências extras (Java/NLTK data); se indisponível, o campo retorna `NaN`.  \n",
    "- Para avaliações extensas, considere normalizar caixa, remover tags, e aplicar **tokenização consistente** entre hipóteses e referências.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu  = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf  = evaluate.load(\"chrf\")\n",
    "try:\n",
    "    metric_meteor = evaluate.load(\"meteor\")\n",
    "    HAS_METEOR = True\n",
    "except Exception:\n",
    "    HAS_METEOR = False\n",
    "\n",
    "def generate_batch(prompts: List[str]) -> List[str]:\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **GEN_KWARGS,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    # TinyLlama-Chat: a resposta vem após o add_generation_prompt; extrair só a parte do assistant\n",
    "    # Heurística simples: cortar pela última ocorrência do chat template do 'assistant' (ou 'Resposta:')\n",
    "    generations = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        full = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        # Se o prompt contém \"Question:\"/ \"Pergunta:\", pegue o que vem depois:\n",
    "        split_key = \"Question:\" if LANG == \"en\" else \"Pergunta:\"\n",
    "        text = full.split(split_key)[-1] if split_key in full else full\n",
    "        # Corte em \"Assistant:\" se o template inserir essa âncora; caso contrário, use tudo\n",
    "        if \"Assistant:\" in text:\n",
    "            text = text.split(\"Assistant:\")[-1]\n",
    "        generations.append(text.strip())\n",
    "    return generations\n",
    "\n",
    "def evaluate_split(ds: Dataset, max_samples: int = 1000) -> Dict[str, float]:\n",
    "    n = min(max_samples, len(ds))\n",
    "    refs, hyps = [], []\n",
    "    for i in tqdm(range(0, n, 16), desc=f\"Avaliando ({n} amostras)\"):\n",
    "        batch = ds[i:i+16]\n",
    "        hyps.extend(generate_batch(batch[\"prompt\"]))\n",
    "        refs.extend(batch[\"reference\"])\n",
    "\n",
    "    rouge = metric_rouge.compute(predictions=hyps, references=refs, use_stemmer=True)\n",
    "    bleu  = metric_bleu.compute(predictions=hyps, references=[[r] for r in refs])\n",
    "    chrf  = metric_chrf.compute(predictions=hyps, references=refs)\n",
    "    meteor_score = (metric_meteor.compute(predictions=hyps, references=refs)[\"meteor\"]\n",
    "                    if HAS_METEOR else float(\"nan\"))\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge[\"rouge1\"], \"rouge2\": rouge[\"rouge2\"], \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"sacrebleu\": bleu[\"score\"], \"chrf\": chrf[\"score\"], \"meteor\": meteor_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be9cc4",
   "metadata": {},
   "source": [
    "### Avaliação final: Perplexidade (targets-only), métricas em validação/teste e salvamento de resultados\n",
    "\n",
    "Este bloco, Liberum, executa a **avaliação completa** do modelo: calcula **Perplexidade** mascarando o prompt (somente sobre a referência), mede **ROUGE/SacreBLEU/chrF/METEOR** nos splits de **validação** e **teste**, agrega tudo em um dicionário e salva em `metrics.json`, registrando também metadados do experimento e o tempo total.\n",
    "\n",
    "**Entradas**\n",
    "- `tok_val_masked`: dataset de validação tokenizado com `labels=-100` no prompt.\n",
    "- `dataset_chat[\"validation\"]` e `dataset_chat[\"test\"]` (campos `prompt`/`reference`).\n",
    "- Hiperparâmetros e configs já definidos: `VAL_SAMPLES`, `TEST_SAMPLES`, `MAX_LEN`, `GEN_KWARGS`, `BASE_MODEL`, `ADAPTER_PATH`, `LANG`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Cronometragem**  \n",
    "   - Inicia `start = time.time()` para medir a duração total.\n",
    "2. **Perplexidade (targets-only)**  \n",
    "   - (Opcional) Subamostra via `select(...)`; aqui usa até `range(8000)` (todos os itens disponíveis da validação).  \n",
    "   - Chama `compute_perplexity_masked(subset, batch_size=4)` e obtém `{\"val_loss\", \"perplexity\"}`.\n",
    "3. **Métricas na validação**  \n",
    "   - `val_metrics = evaluate_split(..., max_samples=VAL_SAMPLES)`: calcula **ROUGE-1/2/L**, **SacreBLEU**, **chrF** e **METEOR** (se disponível).\n",
    "4. **Métricas no teste**  \n",
    "   - `test_metrics = evaluate_split(..., max_samples=TEST_SAMPLES)` com as mesmas métricas.\n",
    "5. **Agregação e metadados**  \n",
    "   - Monta `all_metrics` combinando métricas de validação (incluindo PPL), de teste e um bloco `meta` com:\n",
    "     - `base_model`, `adapter_path`, `gen_kwargs`, `max_len_ctx`, `val_samples`, `test_samples`, `seed`, `lang`, `ppl_targets_only=True`.\n",
    "6. **Persistência**  \n",
    "   - Salva `all_metrics` em **`metrics.json`** (UTF-8, identado).\n",
    "7. **Relato final**  \n",
    "   - Imprime o tempo decorrido (minutos) e retorna/mostra `all_metrics`.\n",
    "\n",
    "**Saídas**\n",
    "- Arquivo `metrics.json` no diretório atual com:\n",
    "  - `validation`: ROUGE/SacreBLEU/chrF/METEOR + `val_loss` e `perplexity`.  \n",
    "  - `test`: ROUGE/SacreBLEU/chrF/METEOR.  \n",
    "  - `meta`: configurações usadas na execução.\n",
    "- Impressões no console com progresso e duração total.\n",
    "\n",
    "**Observações**\n",
    "- **Subamostragem**: para rodar mais rápido, reduza `VAL_SAMPLES/TEST_SAMPLES` e/ou a seleção do subset de PPL.  \n",
    "- **PPL targets-only**: mede a qualidade de modelagem **apenas da resposta**, não do prompt (graças aos `-100`).  \n",
    "- **Reprodutibilidade**: garantida pela `seed`; evite alterar `GEN_KWARGS` se quiser comparabilidade entre execuções.  \n",
    "- **Desempenho**: ajuste `batch_size`/`MAX_LEN` em GPUs com menos VRAM para evitar OOM.  \n",
    "- **Interpretação**: ROUGE e chrF olham sobreposição; SacreBLEU é mais rígido; METEOR (se ativo) pondera sinônimos/stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf0fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexidade (targets-only): 100%|██████████| 2000/2000 [05:34<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Avaliando conjunto de validação...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avaliando (512 amostras):   0%|          | 0/32 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Avaliando (512 amostras): 100%|██████████| 32/32 [08:37<00:00, 16.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Avaliando conjunto de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avaliando (512 amostras): 100%|██████████| 32/32 [08:36<00:00, 16.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Concluído em 22.91 min.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validation': {'rouge1': np.float64(0.11451536666927262),\n",
       "  'rouge2': np.float64(0.033575988489244556),\n",
       "  'rougeL': np.float64(0.08437911788180151),\n",
       "  'sacrebleu': 1.778428374749,\n",
       "  'chrf': 14.480274127841566,\n",
       "  'meteor': np.float64(0.08817447109161045),\n",
       "  'val_loss': 2.6654058856368064,\n",
       "  'perplexity': 14.373782474018604},\n",
       " 'test': {'rouge1': np.float64(0.11288198600130454),\n",
       "  'rouge2': np.float64(0.037328557479277644),\n",
       "  'rougeL': np.float64(0.08489180838075167),\n",
       "  'sacrebleu': 2.3524919534353073,\n",
       "  'chrf': 13.636847210005303,\n",
       "  'meteor': np.float64(0.08694649118272195)},\n",
       " 'meta': {'base_model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       "  'adapter_path': './models/out-sft/final',\n",
       "  'gen_kwargs': {'max_new_tokens': 256,\n",
       "   'temperature': 0.2,\n",
       "   'top_p': 0.9,\n",
       "   'do_sample': False},\n",
       "  'max_len_ctx': 512,\n",
       "  'val_samples': 512,\n",
       "  'test_samples': 512,\n",
       "  'seed': 42,\n",
       "  'lang': 'en',\n",
       "  'ppl_targets_only': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Perplexidade (targets-only) — pode subamostrar se quiser mais rápido:\n",
    "# Ex: subset = tok_val_masked.select(random.sample(range(len(tok_val_masked)), min(2000, len(tok_val_masked))))\n",
    "subset = tok_val_masked.select(range(8000))  # usa tudo da validação\n",
    "ppl_masked = compute_perplexity_masked(subset, batch_size=4)\n",
    "\n",
    "print(\"\\n📊 Avaliando conjunto de validação...\")\n",
    "val_metrics  = evaluate_split(dataset_chat[\"validation\"], max_samples=VAL_SAMPLES)\n",
    "\n",
    "print(\"\\n📈 Avaliando conjunto de teste...\")\n",
    "test_metrics = evaluate_split(dataset_chat[\"test\"],        max_samples=TEST_SAMPLES)\n",
    "\n",
    "all_metrics = {\n",
    "    \"validation\": {**val_metrics, **ppl_masked},\n",
    "    \"test\": test_metrics,\n",
    "    \"meta\": {\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"adapter_path\": ADAPTER_PATH,\n",
    "        \"gen_kwargs\": GEN_KWARGS,\n",
    "        \"max_len_ctx\": MAX_LEN,\n",
    "        \"val_samples\": VAL_SAMPLES,\n",
    "        \"test_samples\": TEST_SAMPLES,\n",
    "        \"seed\": 42,\n",
    "        \"lang\": LANG,\n",
    "        \"ppl_targets_only\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ Concluído em {(time.time()-start)/60:.2f} min.\")\n",
    "all_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4fa43",
   "metadata": {},
   "source": [
    "### Amostragem qualitativa: geração e exportação de exemplos comparativos\n",
    "\n",
    "Este bloco, Liberum, gera **exemplos qualitativos** da performance do modelo — comparando lado a lado o **prompt**, a **resposta gerada** e a **referência humana** — permitindo inspeção manual e revisão externa.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[\"validation\"]`: contém colunas `prompt` e `reference`.\n",
    "- Função `generate_batch`: já definida para gerar respostas a partir de uma lista de prompts.\n",
    "- Parâmetros:\n",
    "  - `k`: número de exemplos a gerar (padrão = 5).\n",
    "  - `seed`: semente para amostragem reprodutível.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Seleção de amostra aleatória**  \n",
    "   - Usa `random.sample` com `seed` fixo para escolher `k` índices da validação.  \n",
    "   - Extrai `prompts` e `references` correspondentes.\n",
    "2. **Geração das respostas**  \n",
    "   - Itera sobre cada `prompt` (com barra de progresso `tqdm`).  \n",
    "   - Usa `generate_batch([prompt])` para obter a resposta do modelo (`preds`).\n",
    "3. **Construção dos registros**  \n",
    "   - Cria lista de dicionários com:\n",
    "     - `\"id\"` — índice sequencial.  \n",
    "     - `\"prompt\"` — texto de entrada.  \n",
    "     - `\"gerado\"` — resposta do modelo.  \n",
    "     - `\"referencia\"` — descrição original.\n",
    "   - Retorna a lista `rows`.\n",
    "4. **Impressão formatada dos exemplos**  \n",
    "   - Exibe cada exemplo separando com `=` para legibilidade.  \n",
    "   - Mostra:\n",
    "     - 🧩 número do exemplo  \n",
    "     - 📥 *Prompt*  \n",
    "     - 🤖 *Texto gerado*  \n",
    "     - 🎯 *Referência original*\n",
    "5. **Exportação para análise externa**  \n",
    "   - Salva os exemplos em `qualitative_samples_val.jsonl`, com uma linha JSON por exemplo.\n",
    "   - Reforça a presença também do arquivo de métricas `metrics.json` (do bloco anterior).\n",
    "\n",
    "**Saídas**\n",
    "- Arquivo `qualitative_samples_val.jsonl` com `prompt`, `gerado`, `referencia` (UTF-8, não escapado).  \n",
    "- Impressão de `k` exemplos no console para análise rápida.  \n",
    "- Retorno de `rows` como lista Python de exemplos.\n",
    "\n",
    "**Observações**\n",
    "- A avaliação qualitativa é crucial para interpretar **qualitativamente** as métricas automáticas (ROUGE/BLUE/chrF).  \n",
    "- Útil para revisores humanos verificarem **fidelidade ao prompt** e **qualidade linguística**.  \n",
    "- Pode ser adaptado para exportar para CSV ou HTML, facilitando revisão colaborativa.  \n",
    "- Se o modelo usa templates de chat diferentes, ajustar a heurística de extração em `generate_batch` melhora a legibilidade das respostas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd74a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gerando 5 exemplos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando exemplos: 100%|██████████| 5/5 [00:38<00:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "🧩 Exemplo 1\n",
      "\n",
      "📥 PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Strawberry Shortcake Baby-girls Infant 2 Piece Polka Dot Legging Set, Yellow, 18 Months\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "🤖 GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Strawberry Shortcake Baby-girls Infant 2 Piece Polka Dot Legging Set, Yellow, 18 Months (Item # 180000000000)Description:This set includes a pair of shorts with polka dot print and a matching top. The shorts are made of 100% cotton and the top is made of 100% cotton. The shorts have a drawstring waist and the top has a snap closure. The set is machine washable.\n",
      "\n",
      "🎯 REFERÊNCIA:\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "🧩 Exemplo 2\n",
      "\n",
      "📥 PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Maxam&reg; 52oz Stainless Steel Oversized Mug\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "🤖 GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Maxam 52oz Stainless Steel Oversized Mug (100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "🎯 REFERÊNCIA:\n",
      " Features plastic liner, screw top, flipper opening, and sure-grip handle. Stands 7-3/4 inches tall, 5 inches in diameter. Limited lifetime warranty. Gift boxed.\n",
      "\n",
      "====================================================================================================\n",
      "🧩 Exemplo 3\n",
      "\n",
      "📥 PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Tabletop Storage 3 Piece Cotton Plate Case Set\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "🤖 GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product Description: Tabletop Storage 3 Piece Cotton Plate Case Set. This set includes 3 pieces of cotton plates, each measuring 10\" x 10\". The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining\n",
      "\n",
      "🎯 REFERÊNCIA:\n",
      " Richards Homewares - 6099 Features: -Material: Canvas.-Protects complete service for 12.-Quilted for added protection.-Easy access self-correcting. Includes: -Includes plate dividers and nylon zippers. Color/Finish: -Finish: Natural. Collection: -Tabletop Storage collection.\n",
      "\n",
      "====================================================================================================\n",
      "🧩 Exemplo 4\n",
      "\n",
      "📥 PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: \n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "🤖 GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Title: The Pure Essence of the Heart: A Guide to the Heart Chakra and the Power of Love (Heart Chakra Series)\n",
      "\n",
      "🎯 REFERÊNCIA:\n",
      " Replace a worn-out or lost battery for your Nintendo DS Lite with this rechargeable Li-ion battery pack.***Please note: this is a generic/aftermarket item. Compatible only with Nintendo DS Lite.***\n",
      "\n",
      "====================================================================================================\n",
      "🧩 Exemplo 5\n",
      "\n",
      "📥 PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Taster's Choice Original House Blend Instant Coffee, 10 Ounce Canister\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "🤖 GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Taster's Choice Original House Blend Instant Coffee, 10 Ounce Canister\n",
      "\n",
      "🎯 REFERÊNCIA:\n",
      " NESCAF&#xC9; TASTER'S CHOICE HOUSE BLEND is 100% pure gourmet coffee made with the finest blend of carefully selected coffee beans. It's the instant coffee that offers you the best way to indulge in pure coffee pleasure whenever, wherever you want to because it's made for coffee lovers just like you.\n",
      "\n",
      "Arquivos exportados: metrics.json, qualitative_samples_val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def show_examples(ds: Dataset, k: int = 5, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    idxs = random.sample(range(len(ds)), k)\n",
    "    samples = ds.select(idxs)\n",
    "    prompts = samples[\"prompt\"]\n",
    "    references = samples[\"reference\"]\n",
    "\n",
    "    print(f\"\\nGerando {k} exemplos...\\n\")\n",
    "\n",
    "    preds = []\n",
    "    for prompt in tqdm(prompts, desc=\"Gerando exemplos\"):\n",
    "        preds.extend(generate_batch([prompt]))\n",
    "\n",
    "    rows = []\n",
    "    for i, (p, g, r) in enumerate(zip(prompts, preds, references)):\n",
    "        rows.append({\n",
    "            \"id\": i,\n",
    "            \"prompt\": p,\n",
    "            \"gerado\": g,\n",
    "            \"referencia\": r\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "qual_val = show_examples(dataset_chat[\"validation\"], k=5)\n",
    "\n",
    "for row in qual_val:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"🧩 Exemplo {row['id']+1}\")\n",
    "    print(\"\\n📥 PROMPT:\\n\", row[\"prompt\"])\n",
    "    print(\"\\n🤖 GERADO:\\n\", row[\"gerado\"])\n",
    "    print(\"\\n🎯 REFERÊNCIA:\\n\", row[\"referencia\"])\n",
    "\n",
    "# Exporta para revisão externa\n",
    "with open(\"qualitative_samples_val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in qual_val:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "print(\"\\nArquivos exportados: metrics.json, qualitative_samples_val.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b7ea0",
   "metadata": {},
   "source": [
    "### Leitura, resumo e interpretação das métricas finais de desempenho\n",
    "\n",
    "Este bloco, Liberum, carrega o arquivo `metrics.json` gerado anteriormente, exibe os principais valores numéricos de **validação** e **teste**, e apresenta um breve guia para interpretar corretamente as métricas.\n",
    "\n",
    "**Entradas**\n",
    "- Arquivo `metrics.json` criado no bloco de avaliação final.  \n",
    "  Contém as chaves:\n",
    "  - `\"validation\"` → métricas + perplexidade.\n",
    "  - `\"test\"` → métricas automáticas de geração.\n",
    "  - `\"meta\"` → informações do experimento (modelo, seed, idioma etc.).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Leitura do arquivo**  \n",
    "   - Abre `metrics.json` em UTF-8 e carrega com `json.load(f)`.\n",
    "2. **Filtragem e arredondamento**  \n",
    "   - Seleciona apenas valores numéricos (`float`/`int`) em cada split.  \n",
    "   - Arredonda para 4 casas decimais para legibilidade.  \n",
    "   - Imprime separadamente:\n",
    "     - `== Validation ==`  \n",
    "     - `== Test ==`\n",
    "\n",
    "**Saídas**\n",
    "- Impressão no console dos valores principais arredondados.\n",
    "- Contexto interpretativo para leitura rápida das métricas.\n",
    "\n",
    "**Observações**\n",
    "- **Alta perplexidade** sugere que o modelo ainda não se ajustou bem ao estilo das respostas esperadas.  \n",
    "- **ROUGE/chrF altos e PPL baixo** indicam melhor consistência textual e adequação ao domínio.  \n",
    "- Se `LANG` for alterado para `\"pt\"` e suas referências forem em português, as pontuações tendem a subir (o modelo avalia em idioma consistente).  \n",
    "- Para análises formais, recomenda-se armazenar versões com `seed`, `config` e data de execução.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa4911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Validation ==\n",
      "{'rouge1': 0.1145, 'rouge2': 0.0336, 'rougeL': 0.0844, 'sacrebleu': 1.7784, 'chrf': 14.4803, 'meteor': 0.0882, 'val_loss': 2.6654, 'perplexity': 14.3738}\n",
      "\n",
      "== Test ==\n",
      "{'rouge1': 0.1129, 'rouge2': 0.0373, 'rougeL': 0.0849, 'sacrebleu': 2.3525, 'chrf': 13.6368, 'meteor': 0.0869}\n"
     ]
    }
   ],
   "source": [
    "with open(\"metrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    m = json.load(f)\n",
    "\n",
    "print(\"\\n== Validation ==\")\n",
    "print({k: round(v, 4) for k, v in m[\"validation\"].items() if isinstance(v, (int, float))})\n",
    "print(\"\\n== Test ==\")\n",
    "print({k: round(v, 4) for k, v in m[\"test\"].items() if isinstance(v, (int, float))})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
