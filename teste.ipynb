{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332f6712",
   "metadata": {},
   "source": [
    "### Configura√ß√£o do ambiente de avalia√ß√£o/infer√™ncia e hiperpar√¢metros de gera√ß√£o\n",
    "\n",
    "Este bloco prepara, Liberum, as **constantes e par√¢metros** usados na fase de **avalia√ß√£o** (ou infer√™ncia controlada) do modelo ajustado, com foco em **reprodutibilidade** e **baixo consumo de VRAM** (‚âà8 GB).\n",
    "\n",
    "**Entradas**\n",
    "- N/A (somente define vari√°veis/constantes para uso posterior).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Imports e seed**  \n",
    "   - Carrega bibliotecas de NLP/treinamento (Transformers, Datasets, PEFT, etc.).  \n",
    "   - Define `set_seed(42)` para reprodutibilidade em gera√ß√£o e amostragem de subsets.\n",
    "2. **Modelos e dados**  \n",
    "   - `BASE_MODEL`: nome do modelo base usado como refer√™ncia nas m√©tricas.  \n",
    "   - `ADAPTER_PATH`: caminho do **adapter LoRA** treinado (sa√≠da do SFT).  \n",
    "   - `DATA_PATH`: caminho do dataset de teste (JSON com `title`, `content`).\n",
    "3. **Idioma de refer√™ncia**  \n",
    "   - `LANG = \"en\"` (ajuste para `\"pt\"` se suas refer√™ncias/respostas-alvo estiverem em portugu√™s).\n",
    "4. **Limites amig√°veis para 8 GB de VRAM**  \n",
    "   - `USE_4BIT = True`: habilita configura√ß√£o pensada para quantiza√ß√£o 4-bit (ser√° usada na carga do modelo).  \n",
    "   - `MAX_LEN = 512`: limita o comprimento do prompt/entrada para caber em GPUs menores.  \n",
    "   - `VAL_SAMPLES` e `TEST_SAMPLES`: tamanhos de amostra para acelerar valida√ß√£o/teste.\n",
    "5. **Par√¢metros de gera√ß√£o** (`GEN_KWARGS`)  \n",
    "   - `max_new_tokens=256`: controla o tamanho m√°ximo da resposta gerada.  \n",
    "   - `do_sample=False`: usa **decodifica√ß√£o gulosa** (determin√≠stica).  \n",
    "   - `temperature=0.2`, `top_p=0.9`: definidos, mas **n√£o t√™m efeito quando `do_sample=False`** (ficam inativos).\n",
    "\n",
    "**Sa√≠das**\n",
    "- Vari√°veis globais configuradas (`BASE_MODEL`, `ADAPTER_PATH`, `DATA_PATH`, `LANG`, `USE_4BIT`, `MAX_LEN`, `VAL_SAMPLES`, `TEST_SAMPLES`, `GEN_KWARGS`) prontas para o pr√≥ximo bloco (carregamento do modelo/adapter e execu√ß√£o das m√©tricas).\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- Se seu conjunto de refer√™ncias estiver em **portugu√™s**, altere `LANG` para `\"pt\"` para alinhar a avalia√ß√£o.  \n",
    "- Considere aumentar `MAX_LEN` para 1024 apenas se a GPU suportar sem **OOM**.  \n",
    "- Para respostas mais criativas/variadas, mude para `do_sample=True` e ent√£o `temperature/top_p` passam a ter efeito.  \n",
    "- Garanta que `ADAPTER_PATH` aponte para o diret√≥rio correto do LoRA salvo (ex.: `./models/out-sft/final`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8198311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed)\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "BASE_MODEL   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   \n",
    "ADAPTER_PATH = \"./models/out-sft/final\"              \n",
    "DATA_PATH    = \"./data/tst.json\"  \n",
    "\n",
    "LANG = \"en\"   # \"en\" ou \"pt\"\n",
    "\n",
    "\n",
    "USE_4BIT = True\n",
    "MAX_LEN  = 512            \n",
    "VAL_SAMPLES  = 512      \n",
    "TEST_SAMPLES = 512\n",
    "\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7d3b0",
   "metadata": {},
   "source": [
    "### Carregamento do modelo base + aplica√ß√£o do adapter LoRA (quantiza√ß√£o 4-bit) para infer√™ncia\n",
    "\n",
    "Este bloco monta o **pipeline de infer√™ncia** com foco em **baixo uso de VRAM** (8 GB), habilitando quantiza√ß√£o 4-bit via *bitsandbytes* e aplicando o **adapter LoRA** treinado sobre o modelo base.\n",
    "\n",
    "**Entradas**\n",
    "- `BASE_MODEL`: identificador do modelo base no Hugging Face Hub.\n",
    "- `ADAPTER_PATH`: diret√≥rio do adapter LoRA salvo ap√≥s o SFT.\n",
    "- `USE_4BIT`: habilita/desabilita quantiza√ß√£o 4-bit.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Configura√ß√£o 4-bit (opcional)**  \n",
    "   - Cria `BitsAndBytesConfig` com `nf4`, *double quant* e `compute_dtype=torch.float16`, reduzindo significativamente a VRAM.\n",
    "2. **Tokenizer**  \n",
    "   - `padding_side=\"left\"` (recomendado para *causal LM* em gera√ß√£o batelada).  \n",
    "   - Define `pad_token = eos_token` caso inexistente, evitando avisos/erros no *batching*.\n",
    "3. **Dicas de performance**  \n",
    "   - `torch.backends.cuda.matmul.allow_tf32 = True` habilita TF32 em GPUs Ampere/Hopper para *matmul* em FP32 (acelera opera√ß√µes compat√≠veis).  \n",
    "   - `torch.set_grad_enabled(False)` desativa *autograd* (infer√™ncia pura).\n",
    "4. **Carregamento do modelo base**  \n",
    "   - `AutoModelForCausalLM.from_pretrained(...)` com `torch_dtype` ajustado ao hardware (FP16 com CUDA; FP32 na CPU), `quantization_config=bnb_config` e `device_map=\"auto\"` (coloca o modelo automaticamente no(s) dispositivo(s) dispon√≠vel(is)).\n",
    "5. **Aplica√ß√£o do adapter LoRA**  \n",
    "   - `PeftModel.from_pretrained(base_model, ADAPTER_PATH)` carrega os pesos LoRA no topo do modelo base.  \n",
    "   - `model.eval()` coloca o modelo em modo de avalia√ß√£o (desativa *dropout*).\n",
    "6. **Confirma√ß√£o de dispositivo**  \n",
    "   - Imprime o `device` do primeiro par√¢metro do modelo para checagem r√°pida.\n",
    "\n",
    "**Sa√≠das**\n",
    "- Objeto `model` pronto para gera√ß√£o (ex.: `model.generate(...)`) com quantiza√ß√£o 4-bit (se habilitada) e LoRA aplicado.\n",
    "- `tokenizer` configurado com *left padding*.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- A quantiza√ß√£o 4-bit depende do pacote **`bitsandbytes`**; garanta que esteja instalado e compat√≠vel com sua GPU/driver.  \n",
    "- TF32 acelera opera√ß√µes FP32; como a infer√™ncia aqui usa FP16, o ganho pode ser limitado, mas a flag √© inofensiva.  \n",
    "- Se ocorrer **OOM**, reduza `MAX_LEN`/`max_new_tokens` ou desative camadas n√£o essenciais.  \n",
    "- Para m√°xima compatibilidade, mantenha o `tokenizer` e o `model` do **mesmo checkpoint base** usado no treinamento do adapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd8e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Quantiza√ß√£o 4-bit (recomendado em 8GB)\n",
    "bnb_config = None\n",
    "if USE_4BIT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, padding_side=\"left\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dicas de perf\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890725c",
   "metadata": {},
   "source": [
    "### Prepara√ß√£o do dataset de avalia√ß√£o e cria√ß√£o de prompts em formato de chat\n",
    "\n",
    "Este bloco, Liberum, carrega o dataset local a partir de JSON, realiza uma divis√£o **estratificada por seed** em `train/validation/test` e constr√≥i um formato de **chat** (system/user ‚Üí assistant) para avalia√ß√£o/gera√ß√£o, produzindo pares `{prompt, reference}`.\n",
    "\n",
    "**Entradas**\n",
    "- `DATA_PATH`: caminho para um JSON com colunas `title` e `content`.\n",
    "- `LANG`: idioma alvo dos prompts (`\"en\"` ou `\"pt\"`).\n",
    "- `tokenizer`: j√° carregado previamente (usado para `apply_chat_template`).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Carga do dataset**  \n",
    "   - `load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")` l√™ o JSON como um √∫nico split.\n",
    "2. **Split determin√≠stico**  \n",
    "   - `train_test_split(test_size=0.1, seed=42)` separa ~10% para **test**.  \n",
    "   - Nova divis√£o no restante para criar **validation** (~10% do restante ‚âà 0.1 total) e **train** (~80% total), via `train_test_split(test_size=0.1111, seed=42)`.  \n",
    "   - Resultado final aproximado: **80% train / 10% validation / 10% test**.\n",
    "3. **Template de chat**  \n",
    "   - `build_chat_prompt(title, lang)` cria mensagens `system` e `user` diferentes conforme `LANG`:\n",
    "     - EN: ‚ÄúWrite a detailed description‚Ä¶‚Äù  \n",
    "     - PT: ‚ÄúEscreva uma descri√ß√£o detalhada‚Ä¶‚Äù\n",
    "   - `tokenizer.apply_chat_template(..., tokenize=False, add_generation_prompt=True)` serializa as mensagens no formato esperado pelo modelo e adiciona o prefixo para a resposta do **assistant**.\n",
    "4. **Mapeamento para avalia√ß√£o**  \n",
    "   - `format_example_chat` transforma cada exemplo em:\n",
    "     - `prompt`: texto de entrada no formato de chat (com t√≠tulo e instru√ß√µes).\n",
    "     - `reference`: o `content` original, usado como refer√™ncia para m√©tricas/checagem de qualidade.\n",
    "   - Aplica o `map` em cada split e monta `dataset_chat`.\n",
    "5. **Inspe√ß√£o**  \n",
    "   - Visualiza o primeiro item de `validation` para checagem r√°pida do schema.\n",
    "\n",
    "**Sa√≠das**\n",
    "- `dataset`: `DatasetDict` com `train`, `validation`, `test`.\n",
    "- `dataset_chat`: `DatasetDict` com campos `prompt` e `reference` em cada split.\n",
    "- Um exemplo impresso de `dataset_chat[\"validation\"][0]` para verifica√ß√£o.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- A seed (`42`) garante **reprodutibilidade** do split.  \n",
    "- `add_generation_prompt=True` √© importante para modelos chat-based que esperam o marcador de in√≠cio da fala do **assistant**.  \n",
    "- Certifique-se de que os t√≠tulos (`title`) sejam informativos; prompts gen√©ricos podem degradar a avalia√ß√£o.  \n",
    "- O `reference` serve como ‚Äúground truth‚Äù textual; m√©tricas do tipo ROUGE/BLEU/BERTScore podem ser aplicadas em etapa posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9a3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 776199/776199 [01:08<00:00, 11319.11 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97014/97014 [00:08<00:00, 11233.19 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97024/97024 [00:08<00:00, 11292.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'uid': 'B00CHH2SG8',\n",
       " 'title': 'Crazy Bananas 6 Pieces Candy Scented Nail Polish Set, (Cherry, Blueberry, Grape, Strawberry, Banana, Apple)',\n",
       " 'content': 'Crazy Bananas 6 piece candy scented nail polish. Kids fun nail polish. 0.24 fluid ounce each. Great gift.',\n",
       " 'target_ind': [46466,\n",
       "  52639,\n",
       "  56909,\n",
       "  102146,\n",
       "  107626,\n",
       "  110728,\n",
       "  113886,\n",
       "  113964,\n",
       "  276873,\n",
       "  304324,\n",
       "  307464,\n",
       "  344542,\n",
       "  380682,\n",
       "  381982,\n",
       "  392963,\n",
       "  503975,\n",
       "  514450,\n",
       "  558488,\n",
       "  561824,\n",
       "  577354,\n",
       "  587446,\n",
       "  679152,\n",
       "  728168,\n",
       "  1069311],\n",
       " 'target_rel': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'prompt': '<|system|>\\nYou are a helpful assistant that writes detailed product descriptions from a given product title.</s>\\n<|user|>\\nTITLE: Crazy Bananas 6 Pieces Candy Scented Nail Polish Set, (Cherry, Blueberry, Grape, Strawberry, Banana, Apple)\\nQuestion: Write a detailed description for this product.</s>\\n<|assistant|>\\n',\n",
       " 'reference': 'Crazy Bananas 6 piece candy scented nail polish. Kids fun nail polish. 0.24 fluid ounce each. Great gift.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega dataset local (JSON com colunas: title, content)\n",
    "raw = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "# Split train/val/test\n",
    "dataset = raw.train_test_split(test_size=0.1, seed=42)\n",
    "tmp = dataset[\"train\"].train_test_split(test_size=0.1111, seed=42)  # ~10% val, ~80% train\n",
    "dataset = DatasetDict({\n",
    "    \"train\": tmp[\"train\"],\n",
    "    \"validation\": tmp[\"test\"],\n",
    "    \"test\": dataset[\"test\"]\n",
    "})\n",
    "\n",
    "def build_chat_prompt(title: str, lang: str = \"en\") -> str:\n",
    "    if lang == \"en\":\n",
    "        system = \"You are a helpful assistant that writes detailed product descriptions from a given product title.\"\n",
    "        user   = f\"TITLE: {title}\\nQuestion: Write a detailed description for this product.\"\n",
    "    else:  # pt\n",
    "        system = \"Voc√™ √© um assistente que escreve uma descri√ß√£o detalhada do produto a partir do t√≠tulo fornecido.\"\n",
    "        user   = f\"T√çTULO: {title}\\nPergunta: Escreva uma descri√ß√£o detalhada para este produto.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "    # add_generation_prompt=True adiciona o prefixo para a resposta do assistant\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def format_example_chat(example, lang=\"en\"):\n",
    "    return {\n",
    "        \"prompt\": build_chat_prompt(example[\"title\"], lang=lang),\n",
    "        \"reference\": example[\"content\"]\n",
    "    }\n",
    "\n",
    "dataset_chat = DatasetDict({\n",
    "    split: ds.map(lambda ex: format_example_chat(ex, lang=LANG))\n",
    "    for split, ds in dataset.items()\n",
    "})\n",
    "\n",
    "# Visualiza um exemplo\n",
    "dataset_chat[\"validation\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526b9c3",
   "metadata": {},
   "source": [
    "### Tokeniza√ß√£o com m√°scara de r√≥tulos (targets-only) e c√°lculo de Perplexidade\n",
    "\n",
    "Este bloco, Liberum, prepara os exemplos no formato **[prompt + refer√™ncia]** para avaliar o modelo medindo **Perplexidade** apenas sobre o **alvo (refer√™ncia)** ‚Äî o **prompt √© ignorado** na perda via m√°scara de r√≥tulos `-100`.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[\"validation\"]`: com campos `prompt` e `reference`.\n",
    "- `tokenizer`, `model`, `MAX_LEN`, `tokenizer.pad_token_id`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **`tokenize_for_ppl_only_target(batch)`**\n",
    "   - Tokeniza `prompt` (`p_ids`) **sem** adicionar *special tokens* (o chat template j√° cont√©m).\n",
    "   - Tokeniza a `reference` antecedida por um espa√ßo (`\" \" + r`) ‚Üí ajuda na jun√ß√£o de tokens.\n",
    "   - Concatena `ids = p_ids + r_ids` e **trunca** para `MAX_LEN`.\n",
    "   - Cria `attention_mask` = 1 para todos os tokens v√°lidos.\n",
    "   - Monta `labels` com **`-100` para os tokens do prompt** (ignorados na loss) e **`r_ids`** como alvos. Ajusta o comprimento para casar com `ids`.\n",
    "   - Retorna `input_ids`, `attention_mask`, `labels`.\n",
    "2. **Mapeamento do split de valida√ß√£o**\n",
    "   - Aplica a fun√ß√£o acima com `batched=True`; remove colunas originais, gerando `tok_val_masked`.\n",
    "3. **Padding din√¢mico custom (`pad_to_max`)**\n",
    "   - Calcula o comprimento m√°ximo do lote e **preenche**:\n",
    "     - `input_ids` com `pad_id`.\n",
    "     - `attention_mask` com `0` no padding.\n",
    "     - `labels` com `-100` no padding (mant√©m m√°scara de perda coerente).\n",
    "4. **`compute_perplexity_masked(tok_dataset, batch_size=4)`**\n",
    "   - Cria `DataLoader` com `collate_fn` que chama `pad_to_max`.\n",
    "   - Faz **infer√™ncia sem gradiente** (`torch.no_grad()`), coletando `out.loss` por batch.\n",
    "   - Calcula `mean_loss` e `perplexity = exp(clamp(mean_loss, 20.0))` (o clamp evita overflow num√©rico).\n",
    "\n",
    "**Sa√≠das**\n",
    "- Dicion√°rio com m√©tricas: `{\"val_loss\": <float>, \"perplexity\": <float>}`.\n",
    "- `tok_val_masked`: dataset tokenizado e rotulado para avalia√ß√£o.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- **Importante**: usar `-100` nos r√≥tulos do prompt garante que a loss/perplexidade me√ßa **apenas** a qualidade da gera√ß√£o da resposta (refer√™ncia), n√£o da instru√ß√£o.\n",
    "- O espa√ßo antes da refer√™ncia evita jun√ß√£o indesejada de tokens (wordpiece/BPE).\n",
    "- Se ocorrer **OOM**, reduza `MAX_LEN` ou `batch_size`.\n",
    "- O padding manual garante consist√™ncia entre `input_ids`, `attention_mask` e `labels` j√° mascarados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/97014 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2052 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97014/97014 [00:22<00:00, 4251.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_for_ppl_only_target(batch):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    refs    = batch[\"reference\"]\n",
    "\n",
    "    input_ids_list, attn_masks_list, labels_list = [], [], []\n",
    "    for p, r in zip(prompts, refs):\n",
    "        # tokenize sem adicionar specials (chat template j√° cont√©m tokens de chat)\n",
    "        p_ids = tokenizer(p, add_special_tokens=False).input_ids\n",
    "        r_ids = tokenizer(\" \" + r, add_special_tokens=False).input_ids  # espa√ßo anteposto ajuda\n",
    "        # concat e trunca\n",
    "        ids = (p_ids + r_ids)[:MAX_LEN]\n",
    "        am  = [1] * len(ids)\n",
    "        # labels: -100 no prompt; r_ids como alvo\n",
    "        labels = ([-100] * min(len(p_ids), len(ids))) + r_ids\n",
    "        labels = labels[:len(ids)]  # garante mesmo comprimento que ids\n",
    "\n",
    "        input_ids_list.append(ids)\n",
    "        attn_masks_list.append(am)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attn_masks_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "tok_val_masked = dataset_chat[\"validation\"].map(\n",
    "    tokenize_for_ppl_only_target, batched=True,\n",
    "    remove_columns=dataset_chat[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "# Padding din√¢mico custom (como j√° temos labels prontos com -100)\n",
    "def pad_to_max(batch, pad_id, label_pad=-100):\n",
    "    maxlen = max(len(x) for x in batch[\"input_ids\"])\n",
    "    def pad(seq, fill, L): return seq + [fill] * (L - len(seq))\n",
    "    input_ids = [pad(x, pad_id, maxlen) for x in batch[\"input_ids\"]]\n",
    "    attn_mask = [pad(x, 0,      maxlen) for x in batch[\"attention_mask\"]]\n",
    "    labels    = [pad(x, label_pad, maxlen) for x in batch[\"labels\"]]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attn_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "def compute_perplexity_masked(tok_dataset, batch_size=4):\n",
    "    loader = DataLoader(\n",
    "        tok_dataset, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=lambda b: pad_to_max(\n",
    "            {k: [d[k] for d in b] for k in b[0].keys()},\n",
    "            pad_id=tokenizer.pad_token_id, label_pad=-100\n",
    "        )\n",
    "    )\n",
    "    losses = []\n",
    "    for batch in tqdm(loader, desc=\"Perplexidade (targets-only)\"):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "        losses.append(out.loss.item())\n",
    "    mean_loss = float(np.mean(losses))\n",
    "    ppl = float(math.exp(min(mean_loss, 20.0)))\n",
    "    return {\"val_loss\": mean_loss, \"perplexity\": ppl}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25b23a",
   "metadata": {},
   "source": [
    "### Gera√ß√£o em lote e avalia√ß√£o autom√°tica com ROUGE, SacreBLEU, chrF e METEOR\n",
    "\n",
    "Este bloco, Liberum, executa **infer√™ncia batelada** a partir dos `prompts` e calcula m√©tricas de **sobreposi√ß√£o n-gram** e **caracteres** para avaliar a qualidade das sa√≠das do modelo em rela√ß√£o √†s refer√™ncias.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[<split>]` com campos `prompt` e `reference`.\n",
    "- `tokenizer`, `model`, `MAX_LEN`, `GEN_KWARGS`, `LANG`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Carregamento de m√©tricas**  \n",
    "   - `rouge`, `sacrebleu`, `chrf` via `evaluate.load`.  \n",
    "   - `meteor` √© opcional (controlado por `HAS_METEOR` ‚Äî pode falhar por depend√™ncias).\n",
    "2. **Gera√ß√£o em lote (`generate_batch`)**  \n",
    "   - Tokeniza `prompts` com **padding** e **truncation** em `MAX_LEN`.  \n",
    "   - Usa `model.generate` com `GEN_KWARGS`, definindo `eos_token_id` e `pad_token_id`.  \n",
    "   - **P√≥s-processamento**: aplica heur√≠stica simples para extrair apenas a resposta do **assistant**:\n",
    "     - Divide pelo marcador da pergunta (`\"Question:\"` ou `\"Pergunta:\"` conforme `LANG`).  \n",
    "     - Tenta cortar ap√≥s `\"Assistant:\"` se existir no template.  \n",
    "     - Faz `strip()` do texto final.\n",
    "3. **Avalia√ß√£o de um split (`evaluate_split`)**  \n",
    "   - Itera sobre o conjunto em minibatches (16 exemplos), chamando `generate_batch`.  \n",
    "   - Agrega hip√≥teses (`hyps`) e refer√™ncias (`refs`).  \n",
    "   - Calcula:\n",
    "     - **ROUGE** (`rouge1`, `rouge2`, `rougeL`) com *stemming*.  \n",
    "     - **SacreBLEU** (formato requer `[[ref]]` por hip√≥tese).  \n",
    "     - **chrF** (n√≠vel de caracteres).  \n",
    "     - **METEOR** (se dispon√≠vel).\n",
    "   - Retorna dicion√°rio com as pontua√ß√µes agregadas.\n",
    "\n",
    "**Sa√≠das**\n",
    "- Dicion√°rio com m√©tricas:  \n",
    "  `{\"rouge1\", \"rouge2\", \"rougeL\", \"sacrebleu\", \"chrf\", \"meteor\"}`.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- A heur√≠stica de recorte pode variar conforme o **chat template** do modelo; ajuste as √¢ncoras se o checkpoint usar marcadores diferentes.  \n",
    "- Se `do_sample=False` em `GEN_KWARGS`, a gera√ß√£o √© **determin√≠stica** (√∫til para comparabilidade entre runs).  \n",
    "- Ajuste `MAX_LEN` caso ocorram truncamentos severos do prompt (pode afetar as m√©tricas).  \n",
    "- `METEOR` pode exigir depend√™ncias extras (Java/NLTK data); se indispon√≠vel, o campo retorna `NaN`.  \n",
    "- Para avalia√ß√µes extensas, considere normalizar caixa, remover tags, e aplicar **tokeniza√ß√£o consistente** entre hip√≥teses e refer√™ncias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu  = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf  = evaluate.load(\"chrf\")\n",
    "try:\n",
    "    metric_meteor = evaluate.load(\"meteor\")\n",
    "    HAS_METEOR = True\n",
    "except Exception:\n",
    "    HAS_METEOR = False\n",
    "\n",
    "def generate_batch(prompts: List[str]) -> List[str]:\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **GEN_KWARGS,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    # TinyLlama-Chat: a resposta vem ap√≥s o add_generation_prompt; extrair s√≥ a parte do assistant\n",
    "    # Heur√≠stica simples: cortar pela √∫ltima ocorr√™ncia do chat template do 'assistant' (ou 'Resposta:')\n",
    "    generations = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        full = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        # Se o prompt cont√©m \"Question:\"/ \"Pergunta:\", pegue o que vem depois:\n",
    "        split_key = \"Question:\" if LANG == \"en\" else \"Pergunta:\"\n",
    "        text = full.split(split_key)[-1] if split_key in full else full\n",
    "        # Corte em \"Assistant:\" se o template inserir essa √¢ncora; caso contr√°rio, use tudo\n",
    "        if \"Assistant:\" in text:\n",
    "            text = text.split(\"Assistant:\")[-1]\n",
    "        generations.append(text.strip())\n",
    "    return generations\n",
    "\n",
    "def evaluate_split(ds: Dataset, max_samples: int = 1000) -> Dict[str, float]:\n",
    "    n = min(max_samples, len(ds))\n",
    "    refs, hyps = [], []\n",
    "    for i in tqdm(range(0, n, 16), desc=f\"Avaliando ({n} amostras)\"):\n",
    "        batch = ds[i:i+16]\n",
    "        hyps.extend(generate_batch(batch[\"prompt\"]))\n",
    "        refs.extend(batch[\"reference\"])\n",
    "\n",
    "    rouge = metric_rouge.compute(predictions=hyps, references=refs, use_stemmer=True)\n",
    "    bleu  = metric_bleu.compute(predictions=hyps, references=[[r] for r in refs])\n",
    "    chrf  = metric_chrf.compute(predictions=hyps, references=refs)\n",
    "    meteor_score = (metric_meteor.compute(predictions=hyps, references=refs)[\"meteor\"]\n",
    "                    if HAS_METEOR else float(\"nan\"))\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge[\"rouge1\"], \"rouge2\": rouge[\"rouge2\"], \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"sacrebleu\": bleu[\"score\"], \"chrf\": chrf[\"score\"], \"meteor\": meteor_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be9cc4",
   "metadata": {},
   "source": [
    "### Avalia√ß√£o final: Perplexidade (targets-only), m√©tricas em valida√ß√£o/teste e salvamento de resultados\n",
    "\n",
    "Este bloco, Liberum, executa a **avalia√ß√£o completa** do modelo: calcula **Perplexidade** mascarando o prompt (somente sobre a refer√™ncia), mede **ROUGE/SacreBLEU/chrF/METEOR** nos splits de **valida√ß√£o** e **teste**, agrega tudo em um dicion√°rio e salva em `metrics.json`, registrando tamb√©m metadados do experimento e o tempo total.\n",
    "\n",
    "**Entradas**\n",
    "- `tok_val_masked`: dataset de valida√ß√£o tokenizado com `labels=-100` no prompt.\n",
    "- `dataset_chat[\"validation\"]` e `dataset_chat[\"test\"]` (campos `prompt`/`reference`).\n",
    "- Hiperpar√¢metros e configs j√° definidos: `VAL_SAMPLES`, `TEST_SAMPLES`, `MAX_LEN`, `GEN_KWARGS`, `BASE_MODEL`, `ADAPTER_PATH`, `LANG`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Cronometragem**  \n",
    "   - Inicia `start = time.time()` para medir a dura√ß√£o total.\n",
    "2. **Perplexidade (targets-only)**  \n",
    "   - (Opcional) Subamostra via `select(...)`; aqui usa at√© `range(8000)` (todos os itens dispon√≠veis da valida√ß√£o).  \n",
    "   - Chama `compute_perplexity_masked(subset, batch_size=4)` e obt√©m `{\"val_loss\", \"perplexity\"}`.\n",
    "3. **M√©tricas na valida√ß√£o**  \n",
    "   - `val_metrics = evaluate_split(..., max_samples=VAL_SAMPLES)`: calcula **ROUGE-1/2/L**, **SacreBLEU**, **chrF** e **METEOR** (se dispon√≠vel).\n",
    "4. **M√©tricas no teste**  \n",
    "   - `test_metrics = evaluate_split(..., max_samples=TEST_SAMPLES)` com as mesmas m√©tricas.\n",
    "5. **Agrega√ß√£o e metadados**  \n",
    "   - Monta `all_metrics` combinando m√©tricas de valida√ß√£o (incluindo PPL), de teste e um bloco `meta` com:\n",
    "     - `base_model`, `adapter_path`, `gen_kwargs`, `max_len_ctx`, `val_samples`, `test_samples`, `seed`, `lang`, `ppl_targets_only=True`.\n",
    "6. **Persist√™ncia**  \n",
    "   - Salva `all_metrics` em **`metrics.json`** (UTF-8, identado).\n",
    "7. **Relato final**  \n",
    "   - Imprime o tempo decorrido (minutos) e retorna/mostra `all_metrics`.\n",
    "\n",
    "**Sa√≠das**\n",
    "- Arquivo `metrics.json` no diret√≥rio atual com:\n",
    "  - `validation`: ROUGE/SacreBLEU/chrF/METEOR + `val_loss` e `perplexity`.  \n",
    "  - `test`: ROUGE/SacreBLEU/chrF/METEOR.  \n",
    "  - `meta`: configura√ß√µes usadas na execu√ß√£o.\n",
    "- Impress√µes no console com progresso e dura√ß√£o total.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- **Subamostragem**: para rodar mais r√°pido, reduza `VAL_SAMPLES/TEST_SAMPLES` e/ou a sele√ß√£o do subset de PPL.  \n",
    "- **PPL targets-only**: mede a qualidade de modelagem **apenas da resposta**, n√£o do prompt (gra√ßas aos `-100`).  \n",
    "- **Reprodutibilidade**: garantida pela `seed`; evite alterar `GEN_KWARGS` se quiser comparabilidade entre execu√ß√µes.  \n",
    "- **Desempenho**: ajuste `batch_size`/`MAX_LEN` em GPUs com menos VRAM para evitar OOM.  \n",
    "- **Interpreta√ß√£o**: ROUGE e chrF olham sobreposi√ß√£o; SacreBLEU √© mais r√≠gido; METEOR (se ativo) pondera sin√¥nimos/stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf0fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexidade (targets-only): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [05:34<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Avaliando conjunto de valida√ß√£o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avaliando (512 amostras):   0%|          | 0/32 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Avaliando (512 amostras): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [08:37<00:00, 16.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Avaliando conjunto de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avaliando (512 amostras): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [08:36<00:00, 16.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Conclu√≠do em 22.91 min.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validation': {'rouge1': np.float64(0.11451536666927262),\n",
       "  'rouge2': np.float64(0.033575988489244556),\n",
       "  'rougeL': np.float64(0.08437911788180151),\n",
       "  'sacrebleu': 1.778428374749,\n",
       "  'chrf': 14.480274127841566,\n",
       "  'meteor': np.float64(0.08817447109161045),\n",
       "  'val_loss': 2.6654058856368064,\n",
       "  'perplexity': 14.373782474018604},\n",
       " 'test': {'rouge1': np.float64(0.11288198600130454),\n",
       "  'rouge2': np.float64(0.037328557479277644),\n",
       "  'rougeL': np.float64(0.08489180838075167),\n",
       "  'sacrebleu': 2.3524919534353073,\n",
       "  'chrf': 13.636847210005303,\n",
       "  'meteor': np.float64(0.08694649118272195)},\n",
       " 'meta': {'base_model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       "  'adapter_path': './models/out-sft/final',\n",
       "  'gen_kwargs': {'max_new_tokens': 256,\n",
       "   'temperature': 0.2,\n",
       "   'top_p': 0.9,\n",
       "   'do_sample': False},\n",
       "  'max_len_ctx': 512,\n",
       "  'val_samples': 512,\n",
       "  'test_samples': 512,\n",
       "  'seed': 42,\n",
       "  'lang': 'en',\n",
       "  'ppl_targets_only': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Perplexidade (targets-only) ‚Äî pode subamostrar se quiser mais r√°pido:\n",
    "# Ex: subset = tok_val_masked.select(random.sample(range(len(tok_val_masked)), min(2000, len(tok_val_masked))))\n",
    "subset = tok_val_masked.select(range(8000))  # usa tudo da valida√ß√£o\n",
    "ppl_masked = compute_perplexity_masked(subset, batch_size=4)\n",
    "\n",
    "print(\"\\nüìä Avaliando conjunto de valida√ß√£o...\")\n",
    "val_metrics  = evaluate_split(dataset_chat[\"validation\"], max_samples=VAL_SAMPLES)\n",
    "\n",
    "print(\"\\nüìà Avaliando conjunto de teste...\")\n",
    "test_metrics = evaluate_split(dataset_chat[\"test\"],        max_samples=TEST_SAMPLES)\n",
    "\n",
    "all_metrics = {\n",
    "    \"validation\": {**val_metrics, **ppl_masked},\n",
    "    \"test\": test_metrics,\n",
    "    \"meta\": {\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"adapter_path\": ADAPTER_PATH,\n",
    "        \"gen_kwargs\": GEN_KWARGS,\n",
    "        \"max_len_ctx\": MAX_LEN,\n",
    "        \"val_samples\": VAL_SAMPLES,\n",
    "        \"test_samples\": TEST_SAMPLES,\n",
    "        \"seed\": 42,\n",
    "        \"lang\": LANG,\n",
    "        \"ppl_targets_only\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Conclu√≠do em {(time.time()-start)/60:.2f} min.\")\n",
    "all_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4fa43",
   "metadata": {},
   "source": [
    "### Amostragem qualitativa: gera√ß√£o e exporta√ß√£o de exemplos comparativos\n",
    "\n",
    "Este bloco, Liberum, gera **exemplos qualitativos** da performance do modelo ‚Äî comparando lado a lado o **prompt**, a **resposta gerada** e a **refer√™ncia humana** ‚Äî permitindo inspe√ß√£o manual e revis√£o externa.\n",
    "\n",
    "**Entradas**\n",
    "- `dataset_chat[\"validation\"]`: cont√©m colunas `prompt` e `reference`.\n",
    "- Fun√ß√£o `generate_batch`: j√° definida para gerar respostas a partir de uma lista de prompts.\n",
    "- Par√¢metros:\n",
    "  - `k`: n√∫mero de exemplos a gerar (padr√£o = 5).\n",
    "  - `seed`: semente para amostragem reprodut√≠vel.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Sele√ß√£o de amostra aleat√≥ria**  \n",
    "   - Usa `random.sample` com `seed` fixo para escolher `k` √≠ndices da valida√ß√£o.  \n",
    "   - Extrai `prompts` e `references` correspondentes.\n",
    "2. **Gera√ß√£o das respostas**  \n",
    "   - Itera sobre cada `prompt` (com barra de progresso `tqdm`).  \n",
    "   - Usa `generate_batch([prompt])` para obter a resposta do modelo (`preds`).\n",
    "3. **Constru√ß√£o dos registros**  \n",
    "   - Cria lista de dicion√°rios com:\n",
    "     - `\"id\"` ‚Äî √≠ndice sequencial.  \n",
    "     - `\"prompt\"` ‚Äî texto de entrada.  \n",
    "     - `\"gerado\"` ‚Äî resposta do modelo.  \n",
    "     - `\"referencia\"` ‚Äî descri√ß√£o original.\n",
    "   - Retorna a lista `rows`.\n",
    "4. **Impress√£o formatada dos exemplos**  \n",
    "   - Exibe cada exemplo separando com `=` para legibilidade.  \n",
    "   - Mostra:\n",
    "     - üß© n√∫mero do exemplo  \n",
    "     - üì• *Prompt*  \n",
    "     - ü§ñ *Texto gerado*  \n",
    "     - üéØ *Refer√™ncia original*\n",
    "5. **Exporta√ß√£o para an√°lise externa**  \n",
    "   - Salva os exemplos em `qualitative_samples_val.jsonl`, com uma linha JSON por exemplo.\n",
    "   - Refor√ßa a presen√ßa tamb√©m do arquivo de m√©tricas `metrics.json` (do bloco anterior).\n",
    "\n",
    "**Sa√≠das**\n",
    "- Arquivo `qualitative_samples_val.jsonl` com `prompt`, `gerado`, `referencia` (UTF-8, n√£o escapado).  \n",
    "- Impress√£o de `k` exemplos no console para an√°lise r√°pida.  \n",
    "- Retorno de `rows` como lista Python de exemplos.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- A avalia√ß√£o qualitativa √© crucial para interpretar **qualitativamente** as m√©tricas autom√°ticas (ROUGE/BLUE/chrF).  \n",
    "- √ötil para revisores humanos verificarem **fidelidade ao prompt** e **qualidade lingu√≠stica**.  \n",
    "- Pode ser adaptado para exportar para CSV ou HTML, facilitando revis√£o colaborativa.  \n",
    "- Se o modelo usa templates de chat diferentes, ajustar a heur√≠stica de extra√ß√£o em `generate_batch` melhora a legibilidade das respostas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd74a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gerando 5 exemplos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando exemplos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:38<00:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üß© Exemplo 1\n",
      "\n",
      "üì• PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Strawberry Shortcake Baby-girls Infant 2 Piece Polka Dot Legging Set, Yellow, 18 Months\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "ü§ñ GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Strawberry Shortcake Baby-girls Infant 2 Piece Polka Dot Legging Set, Yellow, 18 Months (Item # 180000000000)Description:This set includes a pair of shorts with polka dot print and a matching top. The shorts are made of 100% cotton and the top is made of 100% cotton. The shorts have a drawstring waist and the top has a snap closure. The set is machine washable.\n",
      "\n",
      "üéØ REFER√äNCIA:\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "üß© Exemplo 2\n",
      "\n",
      "üì• PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Maxam&reg; 52oz Stainless Steel Oversized Mug\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "ü§ñ GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Maxam 52oz Stainless Steel Oversized Mug (100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "üéØ REFER√äNCIA:\n",
      " Features plastic liner, screw top, flipper opening, and sure-grip handle. Stands 7-3/4 inches tall, 5 inches in diameter. Limited lifetime warranty. Gift boxed.\n",
      "\n",
      "====================================================================================================\n",
      "üß© Exemplo 3\n",
      "\n",
      "üì• PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Tabletop Storage 3 Piece Cotton Plate Case Set\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "ü§ñ GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product Description: Tabletop Storage 3 Piece Cotton Plate Case Set. This set includes 3 pieces of cotton plates, each measuring 10\" x 10\". The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining room. The set is made of cotton and is machine washable. The set is perfect for storing your plates in your kitchen or dining\n",
      "\n",
      "üéØ REFER√äNCIA:\n",
      " Richards Homewares - 6099 Features: -Material: Canvas.-Protects complete service for 12.-Quilted for added protection.-Easy access self-correcting. Includes: -Includes plate dividers and nylon zippers. Color/Finish: -Finish: Natural. Collection: -Tabletop Storage collection.\n",
      "\n",
      "====================================================================================================\n",
      "üß© Exemplo 4\n",
      "\n",
      "üì• PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: \n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "ü§ñ GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Title: The Pure Essence of the Heart: A Guide to the Heart Chakra and the Power of Love (Heart Chakra Series)\n",
      "\n",
      "üéØ REFER√äNCIA:\n",
      " Replace a worn-out or lost battery for your Nintendo DS Lite with this rechargeable Li-ion battery pack.***Please note: this is a generic/aftermarket item. Compatible only with Nintendo DS Lite.***\n",
      "\n",
      "====================================================================================================\n",
      "üß© Exemplo 5\n",
      "\n",
      "üì• PROMPT:\n",
      " <|system|>\n",
      "You are a helpful assistant that writes detailed product descriptions from a given product title.</s>\n",
      "<|user|>\n",
      "TITLE: Taster's Choice Original House Blend Instant Coffee, 10 Ounce Canister\n",
      "Question: Write a detailed description for this product.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "ü§ñ GERADO:\n",
      " Write a detailed description for this product. \n",
      "<|assistant|>\n",
      "Product: Taster's Choice Original House Blend Instant Coffee, 10 Ounce Canister\n",
      "\n",
      "üéØ REFER√äNCIA:\n",
      " NESCAF&#xC9; TASTER'S CHOICE HOUSE BLEND is 100% pure gourmet coffee made with the finest blend of carefully selected coffee beans. It's the instant coffee that offers you the best way to indulge in pure coffee pleasure whenever, wherever you want to because it's made for coffee lovers just like you.\n",
      "\n",
      "Arquivos exportados: metrics.json, qualitative_samples_val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def show_examples(ds: Dataset, k: int = 5, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    idxs = random.sample(range(len(ds)), k)\n",
    "    samples = ds.select(idxs)\n",
    "    prompts = samples[\"prompt\"]\n",
    "    references = samples[\"reference\"]\n",
    "\n",
    "    print(f\"\\nGerando {k} exemplos...\\n\")\n",
    "\n",
    "    preds = []\n",
    "    for prompt in tqdm(prompts, desc=\"Gerando exemplos\"):\n",
    "        preds.extend(generate_batch([prompt]))\n",
    "\n",
    "    rows = []\n",
    "    for i, (p, g, r) in enumerate(zip(prompts, preds, references)):\n",
    "        rows.append({\n",
    "            \"id\": i,\n",
    "            \"prompt\": p,\n",
    "            \"gerado\": g,\n",
    "            \"referencia\": r\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "qual_val = show_examples(dataset_chat[\"validation\"], k=5)\n",
    "\n",
    "for row in qual_val:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üß© Exemplo {row['id']+1}\")\n",
    "    print(\"\\nüì• PROMPT:\\n\", row[\"prompt\"])\n",
    "    print(\"\\nü§ñ GERADO:\\n\", row[\"gerado\"])\n",
    "    print(\"\\nüéØ REFER√äNCIA:\\n\", row[\"referencia\"])\n",
    "\n",
    "# Exporta para revis√£o externa\n",
    "with open(\"qualitative_samples_val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in qual_val:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "print(\"\\nArquivos exportados: metrics.json, qualitative_samples_val.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b7ea0",
   "metadata": {},
   "source": [
    "### Leitura, resumo e interpreta√ß√£o das m√©tricas finais de desempenho\n",
    "\n",
    "Este bloco, Liberum, carrega o arquivo `metrics.json` gerado anteriormente, exibe os principais valores num√©ricos de **valida√ß√£o** e **teste**, e apresenta um breve guia para interpretar corretamente as m√©tricas.\n",
    "\n",
    "**Entradas**\n",
    "- Arquivo `metrics.json` criado no bloco de avalia√ß√£o final.  \n",
    "  Cont√©m as chaves:\n",
    "  - `\"validation\"` ‚Üí m√©tricas + perplexidade.\n",
    "  - `\"test\"` ‚Üí m√©tricas autom√°ticas de gera√ß√£o.\n",
    "  - `\"meta\"` ‚Üí informa√ß√µes do experimento (modelo, seed, idioma etc.).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Leitura do arquivo**  \n",
    "   - Abre `metrics.json` em UTF-8 e carrega com `json.load(f)`.\n",
    "2. **Filtragem e arredondamento**  \n",
    "   - Seleciona apenas valores num√©ricos (`float`/`int`) em cada split.  \n",
    "   - Arredonda para 4 casas decimais para legibilidade.  \n",
    "   - Imprime separadamente:\n",
    "     - `== Validation ==`  \n",
    "     - `== Test ==`\n",
    "\n",
    "**Sa√≠das**\n",
    "- Impress√£o no console dos valores principais arredondados.\n",
    "- Contexto interpretativo para leitura r√°pida das m√©tricas.\n",
    "\n",
    "**Observa√ß√µes**\n",
    "- **Alta perplexidade** sugere que o modelo ainda n√£o se ajustou bem ao estilo das respostas esperadas.  \n",
    "- **ROUGE/chrF altos e PPL baixo** indicam melhor consist√™ncia textual e adequa√ß√£o ao dom√≠nio.  \n",
    "- Se `LANG` for alterado para `\"pt\"` e suas refer√™ncias forem em portugu√™s, as pontua√ß√µes tendem a subir (o modelo avalia em idioma consistente).  \n",
    "- Para an√°lises formais, recomenda-se armazenar vers√µes com `seed`, `config` e data de execu√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa4911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Validation ==\n",
      "{'rouge1': 0.1145, 'rouge2': 0.0336, 'rougeL': 0.0844, 'sacrebleu': 1.7784, 'chrf': 14.4803, 'meteor': 0.0882, 'val_loss': 2.6654, 'perplexity': 14.3738}\n",
      "\n",
      "== Test ==\n",
      "{'rouge1': 0.1129, 'rouge2': 0.0373, 'rougeL': 0.0849, 'sacrebleu': 2.3525, 'chrf': 13.6368, 'meteor': 0.0869}\n"
     ]
    }
   ],
   "source": [
    "with open(\"metrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    m = json.load(f)\n",
    "\n",
    "print(\"\\n== Validation ==\")\n",
    "print({k: round(v, 4) for k, v in m[\"validation\"].items() if isinstance(v, (int, float))})\n",
    "print(\"\\n== Test ==\")\n",
    "print({k: round(v, 4) for k, v in m[\"test\"].items() if isinstance(v, (int, float))})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
