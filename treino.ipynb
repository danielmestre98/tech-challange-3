{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b20242",
   "metadata": {},
   "source": [
    "# Tech Challenge 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294aaf57",
   "metadata": {},
   "source": [
    "### Verificação do ambiente de execução (Python, PyTorch e GPU)\n",
    "\n",
    "Este bloco de código verifica as principais informações do ambiente de execução para garantir que o PyTorch esteja instalado corretamente e que o hardware disponível (como GPU) seja compatível com o processamento acelerado.\n",
    "\n",
    "**Entradas**\n",
    "- Nenhuma entrada direta — o código apenas consulta o ambiente atual.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Importa as bibliotecas necessárias (`torch`, `platform`, `importlib`, `os`).\n",
    "2. Exibe a versão do Python e do PyTorch instaladas.\n",
    "3. Verifica se o PyTorch detecta uma GPU CUDA disponível.\n",
    "4. Caso uma GPU seja encontrada:\n",
    "   - Exibe o nome da GPU.\n",
    "   - Checa se há suporte para operações em formato BF16 (bfloat16), importante para treinamento eficiente em hardware mais recente.\n",
    "5. Caso nenhuma GPU seja detectada, informa que o sistema está rodando apenas em CPU.\n",
    "\n",
    "**Saídas**\n",
    "- Impressão no console das versões do Python e PyTorch.\n",
    "- Status de disponibilidade da GPU CUDA.\n",
    "- Nome da GPU e suporte a BF16 (se aplicável).\n",
    "\n",
    "**Observações**\n",
    "- Útil para depuração de ambiente antes de treinar modelos.\n",
    "- Caso `torch.cuda.is_available()` retorne `False`, o treinamento usará CPU, o que pode reduzir significativamente a performance.\n",
    "- Recomenda-se executar esse bloco logo no início do notebook para garantir compatibilidade de hardware e bibliotecas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9633fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA disponível: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "BF16 suportado: True\n"
     ]
    }
   ],
   "source": [
    "import torch, platform, importlib, os\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    bf16_ok = bool(getattr(torch.cuda, \"is_bf16_supported\", lambda: False)())\n",
    "    print(\"BF16 suportado:\", bf16_ok)\n",
    "else:\n",
    "    print(\"Sem GPU CUDA detectada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc81b61",
   "metadata": {},
   "source": [
    "### Definição dos caminhos, modelo base e hiperparâmetros de treinamento\n",
    "\n",
    "Este bloco configura o ambiente de treinamento, definindo os diretórios, arquivos, modelo base e parâmetros principais do ajuste fino (fine-tuning) do modelo de linguagem.\n",
    "\n",
    "**Entradas**\n",
    "- Arquivos de dados:\n",
    "  - `trn.json`: conjunto de treino.\n",
    "  - `tst.json`: conjunto de teste.\n",
    "- Caminhos e diretórios dentro da pasta `./data` e `./models`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Importa as bibliotecas necessárias (`pathlib`, `os`, `random`).\n",
    "2. Define os caminhos absolutos dos arquivos de treino e teste.\n",
    "3. Cria as pastas de saída:\n",
    "   - `./models/prepared`: onde serão armazenados os dados pré-processados.\n",
    "   - `./models/out-sft`: onde o modelo ajustado será salvo.\n",
    "4. Escolhe um **modelo base pequeno** — `TinyLlama/TinyLlama-1.1B-Chat-v1.0` — compatível com GPUs comuns.  \n",
    "   *(Alternativa: `Qwen/Qwen2.5-1.5B-Instruct`, também leve e otimizada para chat.)*\n",
    "5. Define os **hiperparâmetros de treinamento**, como:\n",
    "   - `num_train_epochs`: número de épocas.\n",
    "   - `batch_size`: tamanho dos lotes por dispositivo.\n",
    "   - `grad_acc_steps`: passos de acumulação de gradiente.\n",
    "   - `learning_rate`: taxa de aprendizado.\n",
    "   - `max_seq_len`: tamanho máximo de sequência.\n",
    "   - Parâmetros LoRA (`lora_r`, `lora_alpha`, `lora_dropout`) para adaptação leve do modelo.\n",
    "   - `seed`: semente de aleatoriedade para reprodutibilidade.\n",
    "6. Inicializa o gerador aleatório (`random.seed`) com a semente definida.\n",
    "7. Exibe no console o modelo selecionado, os caminhos dos arquivos de dados e os diretórios de saída.\n",
    "\n",
    "**Saídas**\n",
    "- Impressão das configurações principais no console.\n",
    "- Criação (se inexistentes) das pastas `./models/prepared` e `./models/out-sft`.\n",
    "\n",
    "**Observações**\n",
    "- O modelo escolhido (`TinyLlama`) é adequado para testes e protótipos rápidos.\n",
    "- Os hiperparâmetros devem ser ajustados conforme a capacidade da GPU disponível.\n",
    "- Garantir que os arquivos `trn.json` e `tst.json` existam antes de prosseguir para evitar erros no pré-processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02e493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_MODEL: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Arquivos: C:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\data\\trn.json | C:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\data\\tst.json\n",
      "Saídas: OUT_DIR= models\\out-sft | PREP_DIR= models\\prepared\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, random\n",
    "\n",
    "# Caminhos dos arquivos que você já tem\n",
    "TRAIN_PATH = Path(\"./data/trn.json\")\n",
    "TEST_PATH  = Path(\"./data/tst.json\")\n",
    "\n",
    "# Pastas de saída\n",
    "OUT_DIR   = Path(\"./models/out-sft\")\n",
    "PREP_DIR  = Path(\"./models/prepared\")\n",
    "PREP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Modelo base pequeno para caber em GPU comum\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # alternativa: \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Hiperparâmetros (ajuste conforme seu hardware)\n",
    "CFG = dict(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    grad_acc_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    max_seq_len=1024,\n",
    "    lora_r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "random.seed(CFG[\"seed\"])\n",
    "\n",
    "print(\"BASE_MODEL:\", BASE_MODEL)\n",
    "print(\"Arquivos:\", TRAIN_PATH.resolve(), \"|\", TEST_PATH.resolve())\n",
    "print(\"Saídas:\", \"OUT_DIR=\", OUT_DIR, \"| PREP_DIR=\", PREP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b4033",
   "metadata": {},
   "source": [
    "### Leitura e validação dos arquivos de dados em formato JSON/JSONL\n",
    "\n",
    "Este bloco carrega os conjuntos de **treinamento** e **teste** a partir de arquivos JSON, com suporte flexível tanto para formato de lista (`.json`) quanto para formato linha a linha (`.jsonl`). Ele garante que os dados sejam lidos corretamente e exibe um exemplo do conteúdo para verificação.\n",
    "\n",
    "**Entradas**\n",
    "- Caminhos definidos anteriormente:\n",
    "  - `TRAIN_PATH` → `./data/trn.json`\n",
    "  - `TEST_PATH` → `./data/tst.json`\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Define a função `read_any_json(path)`:\n",
    "   - Lê o conteúdo do arquivo indicado, forçando codificação UTF-8 e ignorando erros de leitura.\n",
    "   - Primeira tentativa: interpreta o conteúdo completo como uma **lista JSON** (`[{}, {}, ...]`).\n",
    "   - Se falhar, aplica um **fallback** para o formato **JSONL** (um JSON por linha):\n",
    "     - Lê o arquivo linha por linha.\n",
    "     - Ignora linhas vazias ou inválidas.\n",
    "     - Converte cada linha válida em um dicionário e adiciona à lista de resultados.\n",
    "2. Carrega os arquivos de treino e teste usando a função.\n",
    "3. Exibe no console:\n",
    "   - O número total de registros carregados em cada conjunto.\n",
    "   - Um exemplo da primeira entrada do treino, mostrando campos típicos: `uid`, `title`, `content`, `target_ind`, `target_rel`.\n",
    "\n",
    "**Saídas**\n",
    "- Listas Python contendo os dados brutos (`train_raw` e `test_raw`).\n",
    "- Impressão com a contagem e um exemplo representativo dos registros.\n",
    "\n",
    "**Observações**\n",
    "- A função é tolerante a erros e ignora linhas corrompidas, o que facilita o trabalho com bases parcialmente inconsistentes.\n",
    "- Essa abordagem é útil quando não se sabe de antemão se o dataset está em formato JSON puro ou JSONL.\n",
    "- O exemplo impresso serve para inspecionar a estrutura das chaves e validar o schema antes do pré-processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc53faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregado treino bruto: 2248619\n",
      "Carregado teste bruto:  970237\n",
      "Exemplo treino: {'uid': '0000031909', 'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_any_json(path: Path):\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    rows = []\n",
    "\n",
    "    # Tenta lista JSON\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            rows = obj\n",
    "            return rows\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: tenta JSONL\n",
    "    rows = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rows.append(json.loads(line))\n",
    "        except Exception:\n",
    "            # Ignora linhas quebradas\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "train_raw = read_any_json(TRAIN_PATH)\n",
    "test_raw  = read_any_json(TEST_PATH)\n",
    "\n",
    "print(f\"Carregado treino bruto: {len(train_raw)}\")\n",
    "print(f\"Carregado teste bruto:  {len(test_raw)}\")\n",
    "if train_raw:\n",
    "    ex = {k: train_raw[0].get(k) for k in (\"uid\",\"title\",\"content\",\"target_ind\",\"target_rel\")}\n",
    "    print(\"Exemplo treino:\", ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9979968",
   "metadata": {},
   "source": [
    "### Limpeza, normalização e deduplicação dos dados de entrada\n",
    "\n",
    "Este bloco realiza o **pré-processamento textual** dos conjuntos de treino e teste, eliminando registros inválidos, padronizando o texto e removendo duplicatas. O objetivo é garantir que os dados usados no ajuste fino sejam consistentes e livres de ruído.\n",
    "\n",
    "**Entradas**\n",
    "- `train_raw` e `test_raw`: listas de dicionários carregadas anteriormente com os dados brutos.\n",
    "- Diretório de saída `PREP_DIR` (ex.: `./models/prepared`).\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Define um conjunto de strings consideradas nulas (`NULL_STRINGS`), incluindo variações como `\"na\"`, `\"n/a\"`, `\"null\"`, `\"nan\"`, `\"-\"`, etc.\n",
    "2. Cria a função `norm_text(x)`:\n",
    "   - Converte valores numéricos em string (ignorando `NaN`).\n",
    "   - Remove espaços extras e múltiplos espaços entre palavras.\n",
    "   - Retorna uma string limpa e padronizada.\n",
    "3. Define a função `is_bad(s)`:\n",
    "   - Verifica se o texto é nulo, vazio ou contém apenas placeholders genéricos.\n",
    "4. Implementa `clean_rows(rows)`:\n",
    "   - Aplica `norm_text` ao `title` e `content` de cada registro.\n",
    "   - Remove registros sem conteúdo útil.\n",
    "   - Deduplica os dados com base no `uid` (ou `title`, caso o `uid` esteja ausente).\n",
    "5. Aplica `clean_rows` aos conjuntos de treino e teste.\n",
    "6. Exibe estatísticas sobre o processo de limpeza:\n",
    "   - Quantos registros foram removidos.\n",
    "   - Quantos permanecem após deduplicação.\n",
    "7. Salva as versões limpas dos datasets em formato **JSONL**, permitindo auditoria posterior:\n",
    "   - `trn_clean.jsonl`\n",
    "   - `tst_clean.jsonl`\n",
    "8. Mostra um exemplo de registro limpo para inspeção visual.\n",
    "\n",
    "**Saídas**\n",
    "- Arquivos limpos gravados em `PREP_DIR`.\n",
    "- Impressão das contagens de registros removidos e exemplo do resultado final.\n",
    "\n",
    "**Observações**\n",
    "- Essa limpeza previne falhas no tokenizador e evita que textos vazios prejudiquem o treinamento.\n",
    "- O uso de `json.dumps(..., ensure_ascii=False)` preserva acentuação e caracteres Unicode.\n",
    "- Embora o teste geralmente venha pré-processado, aplicá-la garante consistência entre os conjuntos.\n",
    "- É importante realizar auditoria dos dados limpos antes de seguir para a etapa de tokenização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c42de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino após limpeza/dedup: 1390102 (removidos: 858517)\n",
      "Teste após limpeza/dedup:  599631  (removidos: 370606)\n",
      "Exemplo pós-limpeza: {'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length'}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "NULL_STRINGS = {\"\", \" \", \"na\", \"n/a\", \"none\", \"null\", \"nan\", \"-\", \"--\", \"unknown\"}\n",
    "\n",
    "def norm_text(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, (int, float)):\n",
    "        try:\n",
    "            if math.isnan(x):\n",
    "                return \"\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        return str(x)\n",
    "    x = str(x).strip()\n",
    "    # Remove múltiplos espaços\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    return x\n",
    "\n",
    "def is_bad(s: str):\n",
    "    s_norm = s.strip().lower()\n",
    "    return (s_norm in NULL_STRINGS) or (len(s_norm) == 0)\n",
    "\n",
    "def clean_rows(rows):\n",
    "    cleaned = []\n",
    "    for r in rows:\n",
    "        title = norm_text(r.get(\"title\"))\n",
    "        content = norm_text(r.get(\"content\"))\n",
    "        # Filtra registros nulos/inúteis\n",
    "        if is_bad(title) or is_bad(content):\n",
    "            continue\n",
    "        cleaned.append({\n",
    "            \"uid\": r.get(\"uid\"),\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"target_ind\": r.get(\"target_ind\"),\n",
    "            \"target_rel\": r.get(\"target_rel\"),\n",
    "        })\n",
    "    # Deduplicar (prioriza UID, senão título)\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for r in cleaned:\n",
    "        key = r[\"uid\"] if r[\"uid\"] else r[\"title\"]\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(r)\n",
    "    return dedup\n",
    "\n",
    "train = clean_rows(train_raw)\n",
    "test  = clean_rows(test_raw)   # normalmente teste já vem limpo, mas não custa\n",
    "print(f\"Treino após limpeza/dedup: {len(train)} (removidos: {len(train_raw)-len(train)})\")\n",
    "print(f\"Teste após limpeza/dedup:  {len(test)}  (removidos: {len(test_raw)-len(test)})\")\n",
    "\n",
    "# Salvar versões limpas (para auditoria)\n",
    "with (PREP_DIR/\"trn_clean.jsonl\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in train:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with (PREP_DIR/\"tst_clean.jsonl\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in test:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "if train:\n",
    "    print(\"Exemplo pós-limpeza:\", {k: train[0][k] for k in (\"title\",\"content\")})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14f82c",
   "metadata": {},
   "source": [
    "### Geração dos prompts e preparação do dataset para ajuste fino (SFT)\n",
    "\n",
    "Este bloco cria o conjunto de dados formatado para **Supervised Fine-Tuning (SFT)**, transformando os registros limpos em instruções de treinamento com perguntas e respostas simuladas no estilo de chat.\n",
    "\n",
    "**Entradas**\n",
    "- `train`: conjunto de dados limpo.\n",
    "- `CFG[\"seed\"]`: semente de aleatoriedade para reprodutibilidade.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Define o conjunto de **perguntas padrão** (`QUESTION_TEMPLATES`), que simulam interações típicas de e-commerce, como:\n",
    "   - “Descreva o produto.”\n",
    "   - “Liste as principais características.”\n",
    "   - “Resuma a descrição do item.”\n",
    "2. Implementa a função `build_prompt(title, question)`:\n",
    "   - Constrói um prompt no formato **[INST]...[/INST]**, padrão usado por modelos de chat (ex.: Llama, Mistral).\n",
    "   - Inclui o título do produto e a pergunta, instruindo o modelo a responder apenas com base na descrição.\n",
    "3. Define `to_sft_rows(rows)`:\n",
    "   - Para cada item do dataset:\n",
    "     - Escolhe aleatoriamente uma das perguntas.\n",
    "     - Gera um prompt completo unindo instrução + conteúdo (`content`).\n",
    "   - Retorna uma lista de dicionários com a chave `\"text\"`, contendo os exemplos finais de treino.\n",
    "4. Gera o conjunto `sft_train_rows` e aplica divisão interna (holdout) para validação:\n",
    "   - Define a fração de validação (`val_frac`): 5% para datasets grandes, 20% para pequenos.\n",
    "   - Embaralha e separa os exemplos em treino e validação (`sft_val_rows`).\n",
    "5. Cria o objeto `DatasetDict` da biblioteca **Hugging Face Datasets** com duas partições:\n",
    "   - `train`\n",
    "   - `validation`\n",
    "6. Exibe o resumo do dataset e um exemplo truncado do primeiro prompt gerado (primeiros 300 caracteres).\n",
    "\n",
    "**Saídas**\n",
    "- Objeto `dataset` contendo os conjuntos de treino e validação prontos para o fine-tuning.\n",
    "- Exemplo de prompt-formatado exibido no console.\n",
    "\n",
    "**Observações**\n",
    "- Essa estrutura segue o formato esperado por pipelines de ajuste fino da Hugging Face.\n",
    "- O uso de prompts em estilo de chat ajuda o modelo a aprender padrões de pergunta e resposta contextualizados.\n",
    "- A aleatoriedade controlada pela semente (`seed`) garante repetibilidade na geração dos exemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77001f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1320597\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 69505\n",
      "    })\n",
      "})\n",
      "Exemplo SFT: [INST] Você é um assistente de e-commerce. Responda APENAS com informações da descrição.\n",
      "Título do produto: Mighty Peking Man\n",
      "Pergunta: Resuma a descrição do item. [/INST]\n",
      "What makesMighty Peking Mansuch a trashy delight? It's not just the absurdly obvious special effects and atrocious dubbing--thos ...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "random.seed(CFG[\"seed\"])\n",
    "\n",
    "QUESTION_TEMPLATES = [\n",
    "    \"Descreva o produto.\",\n",
    "    \"Liste as principais características.\",\n",
    "    \"Resuma a descrição do item.\",\n",
    "]\n",
    "\n",
    "def build_prompt(title, question):\n",
    "    return (\n",
    "        \"[INST] Você é um assistente de e-commerce. Responda APENAS com informações da descrição.\\n\"\n",
    "        f\"Título do produto: {title}\\nPergunta: {question} [/INST]\"\n",
    "    )\n",
    "\n",
    "def to_sft_rows(rows):\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        q = random.choice(QUESTION_TEMPLATES)\n",
    "        prompt = build_prompt(r[\"title\"], q)\n",
    "        out.append({\"text\": prompt + \"\\n\" + r[\"content\"]})\n",
    "    return out\n",
    "\n",
    "sft_train_rows = to_sft_rows(train)\n",
    "# Usaremos o teste como conjunto de avaliação externa; para validação interna criamos um holdout\n",
    "val_frac = 0.05 if len(sft_train_rows) > 20 else 0.2\n",
    "val_n = max(1, int(len(sft_train_rows)*val_frac))\n",
    "random.shuffle(sft_train_rows)\n",
    "sft_val_rows   = sft_train_rows[:val_n]\n",
    "sft_train_rows = sft_train_rows[val_n:]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(sft_train_rows) if sft_train_rows else Dataset.from_list([{\"text\": \"\"}]),\n",
    "    \"validation\": Dataset.from_list(sft_val_rows) if sft_val_rows else Dataset.from_list([{\"text\": \"\"}]),\n",
    "})\n",
    "print(dataset)\n",
    "print(\"Exemplo SFT:\", dataset[\"train\"][0][\"text\"][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8cb54",
   "metadata": {},
   "source": [
    "### Verificação detalhada do ambiente PyTorch e da GPU CUDA\n",
    "\n",
    "Este bloco realiza uma inspeção completa do ambiente de execução do **PyTorch**, verificando compatibilidade com aceleração via **GPU** e suporte a recursos avançados como **BF16 (bfloat16)**. O objetivo é garantir que o hardware e os drivers estão configurados corretamente antes do treinamento.\n",
    "\n",
    "**Entradas**\n",
    "- Nenhuma entrada direta. O código apenas consulta o sistema local e as bibliotecas instaladas.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. Importa os módulos necessários (`torch`, `importlib`, `platform`).\n",
    "2. Exibe as versões do **Python** e do **PyTorch** instaladas.\n",
    "3. Verifica se há uma GPU compatível com **CUDA** disponível (`torch.cuda.is_available()`).\n",
    "4. Caso exista GPU:\n",
    "   - Mostra o **nome do dispositivo** detectado.\n",
    "   - Obtém as **propriedades da GPU** (`torch.cuda.get_device_properties`).\n",
    "   - Calcula e exibe a **memória total de VRAM** em gigabytes.\n",
    "   - Checa se o ambiente oferece suporte ao formato **BF16**, útil para treinar modelos de forma mais eficiente em GPUs modernas.\n",
    "5. Caso não exista GPU, informa que o treinamento ocorrerá apenas em CPU.\n",
    "\n",
    "**Saídas**\n",
    "- Informações impressas no console:\n",
    "  - Versões do Python e PyTorch.\n",
    "  - Status de disponibilidade da GPU.\n",
    "  - Nome, VRAM e suporte BF16 (se aplicável).\n",
    "\n",
    "**Observações**\n",
    "- Executar este bloco **antes de iniciar o treinamento** ajuda a diagnosticar incompatibilidades com CUDA ou versões incorretas do PyTorch.\n",
    "- O suporte a **BF16** melhora o desempenho e reduz o consumo de memória em GPUs compatíveis (como A100, H100, RTX 40xx).\n",
    "- Caso “Sem GPU CUDA detectada.” seja exibido, é necessário verificar se:\n",
    "  - Os drivers NVIDIA e CUDA Toolkit estão instalados.\n",
    "  - A versão do PyTorch foi compilada com suporte a CUDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c722c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA disponível: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "VRAM total (GB): 8.0\n",
      "BF16 suportado pelo driver/runtime: True\n"
     ]
    }
   ],
   "source": [
    "import torch, importlib, platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(\"VRAM total (GB):\", round(props.total_memory/1024**3, 2))\n",
    "    bf16_ok = bool(getattr(torch.cuda, \"is_bf16_supported\", lambda: False)())\n",
    "    print(\"BF16 suportado pelo driver/runtime:\", bf16_ok)\n",
    "else:\n",
    "    print(\"Sem GPU CUDA detectada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964ba42",
   "metadata": {},
   "source": [
    "### Carregamento do modelo com quantização 4-bit + configuração LoRA e SFT (TRL)\n",
    "\n",
    "Este bloco prepara o pipeline de **fine-tuning supervisionado (SFT)** com economia de memória via **quantização 4-bit (bitsandbytes)** e **adaptação leve LoRA (PEFT)**, deixando o ambiente pronto para iniciar o treinamento.\n",
    "\n",
    "**Entradas**\n",
    "- `BASE_MODEL`: nome do modelo base do Hugging Face Hub.\n",
    "- `CFG`: dicionário de hiperparâmetros (épocas, batch size, seed, etc.).\n",
    "- `dataset[\"train\"]` e `dataset[\"validation\"]`: datasets formatados com a coluna `\"text\"`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Semente**: fixa `torch.manual_seed(CFG[\"seed\"])` para reprodutibilidade.\n",
    "2. **Quantização (4-bit)**: cria `BitsAndBytesConfig` com:\n",
    "   - `load_in_4bit=True` para carregar pesos em 4-bit.\n",
    "   - `bnb_4bit_use_double_quant=True` e `bnb_4bit_quant_type=\"nf4\"` (quantização NF4 mais estável).\n",
    "   - `bnb_4bit_compute_dtype=torch.bfloat16` para computação em **BF16** (ganho de performance/estabilidade em GPUs compatíveis).\n",
    "3. **Tokenizer**:\n",
    "   - Carrega com `AutoTokenizer.from_pretrained(BASE_MODEL)`.\n",
    "   - Ajusta `pad_token = eos_token` se o modelo não tiver `pad_token` (evita warnings/erros em batching).\n",
    "4. **Modelo base quantizado**:\n",
    "   - `AutoModelForCausalLM.from_pretrained(...)` com `quantization_config=bnb`, `device_map=\"auto\"` (distribui automaticamente nos dispositivos) e `torch_dtype=torch.bfloat16`.\n",
    "5. **Configuração LoRA** (`LoraConfig`):\n",
    "   - Usa `r`, `alpha` e `dropout` vindos de `CFG`.\n",
    "   - Aplica LoRA nos módulos de projeção/MLP típicos de LLMs (`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`), reduzindo o número de parâmetros treináveis e o consumo de VRAM.\n",
    "6. **Hiperparâmetros de SFT** (`SFTConfig`):\n",
    "   - Diretório de saída (`output_dir`), épocas, batch sizes, `gradient_accumulation_steps`.\n",
    "   - `learning_rate`, `lr_scheduler_type=\"cosine\"`, `warmup_ratio=0.03`, `weight_decay=0.01`.\n",
    "   - Estratégia de avaliação por passos (`eval_strategy=\"steps\"`, `eval_steps=500`) e checkpoints (`save_steps=500`).\n",
    "   - `max_steps=1000` e `bf16=True` (usa bfloat16 no treino).\n",
    "7. **Formatação de exemplos**:\n",
    "   - `formatting_func` retorna `example[\"text\"]` (cada linha já vem pronta para o modelo).\n",
    "8. **Instancia o treinador** (`SFTTrainer`):\n",
    "   - Recebe `model`, `peft_config` (LoRA), `args` (SFTConfig) e os datasets de treino/validação.\n",
    "9. Imprime confirmação: “Pronto para treinar.”\n",
    "\n",
    "**Saídas**\n",
    "- Objeto `trainer` pronto para `trainer.train()`.\n",
    "- Tokenizer configurado (com `pad_token` definido).\n",
    "- Modelo base carregado com pesos em 4-bit e cabeçalhos LoRA aplicados.\n",
    "\n",
    "**Observações**\n",
    "- **Requisitos**: `bitsandbytes` instalado e GPU com suporte adequado para 4-bit/BF16 (CPU pode não suportar essa configuração).\n",
    "- **Memória**: quantização 4-bit reduz significativamente o uso de VRAM, permitindo treinar modelos maiores em GPUs comuns.\n",
    "- **Pad Token**: usar `eos_token` como `pad_token` é prático, mas garanta que métricas e perda não sejam enviesadas por padding (atenção às máscaras de atenção na etapa de treino).\n",
    "- **Avaliação**: com `eval_strategy=\"steps\"`, a validação ocorre periodicamente; ajuste `eval_steps` e `save_steps` conforme o tamanho do dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd02827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 1320597/1320597 [00:32<00:00, 40759.69 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1320597/1320597 [00:20<00:00, 64298.13 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/1320597 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5203 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train dataset: 100%|██████████| 1320597/1320597 [06:09<00:00, 3577.50 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1320597/1320597 [00:01<00:00, 660370.45 examples/s] \n",
      "Applying formatting function to eval dataset: 100%|██████████| 69505/69505 [00:01<00:00, 55063.57 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 69505/69505 [00:01<00:00, 68008.18 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 69505/69505 [00:19<00:00, 3594.13 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 69505/69505 [00:00<00:00, 1418510.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronto para treinar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "\n",
    "# Quantização 4-bit (requer bitsandbytes)\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=CFG[\"lora_r\"],\n",
    "    lora_alpha=CFG[\"lora_alpha\"],\n",
    "    lora_dropout=CFG[\"lora_dropout\"],\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=CFG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CFG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CFG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CFG[\"grad_acc_steps\"],\n",
    "    learning_rate=CFG[\"learning_rate\"],\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    max_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    seed=CFG[\"seed\"],\n",
    ")\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora,\n",
    "    args=sft_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"Pronto para treinar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b6d85",
   "metadata": {},
   "source": [
    "### Execução do treinamento e salvamento do modelo final\n",
    "\n",
    "Este bloco realiza a etapa final do pipeline: **treinar o modelo ajustado** e **salvar os resultados** para uso posterior ou implantação.\n",
    "\n",
    "**Entradas**\n",
    "- Objeto `trainer` configurado anteriormente, que contém:\n",
    "  - O modelo base com LoRA e quantização aplicadas.\n",
    "  - Os datasets de treino e validação.\n",
    "  - Os parâmetros de otimização definidos em `SFTConfig`.\n",
    "\n",
    "**Processo (passo a passo)**\n",
    "1. **Treinamento supervisionado (`trainer.train()`)**  \n",
    "   Inicia o processo de ajuste fino (fine-tuning) do modelo, com base no dataset preparado.  \n",
    "   Durante a execução:\n",
    "   - O modelo é atualizado apenas nas camadas LoRA (parametrização leve).\n",
    "   - São exibidas métricas de perda (loss) e, se configuradas, métricas de avaliação periódicas.\n",
    "   - Checkpoints intermediários são salvos conforme definido em `save_steps`.\n",
    "2. **Salvamento do modelo ajustado (`trainer.save_model`)**  \n",
    "   Exporta os pesos resultantes do treinamento para o diretório `OUT_DIR/final`.  \n",
    "   Este passo garante que as modificações feitas durante o fine-tuning possam ser recarregadas posteriormente.\n",
    "3. **Salvamento do tokenizer (`tokenizer.save_pretrained`)**  \n",
    "   Salva o tokenizador no mesmo diretório, garantindo compatibilidade com o modelo durante a inferência.\n",
    "\n",
    "**Saídas**\n",
    "- Diretório final `models/out-sft/final/` contendo:\n",
    "  - Arquivos do modelo ajustado (pesos LoRA e configurações).\n",
    "  - Tokenizador (`tokenizer.json`, `special_tokens_map.json`, etc.).\n",
    "- Log de progresso e métricas do treinamento exibidos no console.\n",
    "\n",
    "**Observações**\n",
    "- Após essa etapa, o modelo está pronto para **inferência ou avaliação**, podendo ser carregado via `from_pretrained(\"models/out-sft/final\")`.\n",
    "- O tempo de execução depende do tamanho do dataset, das configurações de batch size e da GPU utilizada.\n",
    "- É boa prática verificar a **perda final (loss)** e testar o modelo em exemplos reais antes de distribuí-lo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3edc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "c:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 3:37:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.774900</td>\n",
       "      <td>1.702535</td>\n",
       "      <td>1.714422</td>\n",
       "      <td>3797894.000000</td>\n",
       "      <td>0.645449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.776200</td>\n",
       "      <td>1.692364</td>\n",
       "      <td>1.696128</td>\n",
       "      <td>7551922.000000</td>\n",
       "      <td>0.647121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\Tech Challenge 3 (1)\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models\\\\out-sft\\\\final\\\\tokenizer_config.json',\n",
       " 'models\\\\out-sft\\\\final\\\\special_tokens_map.json',\n",
       " 'models\\\\out-sft\\\\final\\\\chat_template.jinja',\n",
       " 'models\\\\out-sft\\\\final\\\\tokenizer.model',\n",
       " 'models\\\\out-sft\\\\final\\\\added_tokens.json',\n",
       " 'models\\\\out-sft\\\\final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(str(OUT_DIR / \"final\"))\n",
    "tokenizer.save_pretrained(str(OUT_DIR / \"final\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
